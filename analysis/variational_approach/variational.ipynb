{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnaselar/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##my stuff\n",
    "from imagery_psychophysics.src.stirling_maps import sparse_point_maps as spm\n",
    "from imagery_psychophysics.src.model_z import noise_grid\n",
    "\n",
    "\n",
    "##other people's stuff\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fractions import gcd\n",
    "from PIL.Image import open\n",
    "from PIL import Image\n",
    "from scipy.misc import imresize\n",
    "from os.path import join\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.misc import comb as nCk\n",
    "from time import time\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from theano import function, shared\n",
    "from theano.tensor.extra_ops import repeat, to_one_hot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.mlab import griddata\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In which we add some code to loop over all subjects, images, conditions and save results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Model variable classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility variables needed for various things below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##a theano random number generator\n",
    "rng = MRG_RandomStreams(use_cuda = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##data types\n",
    "floatX = 'float32'\n",
    "intX = 'int32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##sometime we want to convert to 1hot format outside of theano expressions\n",
    "##some compile the theano to_one_hot here\n",
    "_anyVector = T.vector('anyMatrix',dtype=intX)\n",
    "_anyInt = T.scalar('anyInt',dtype=intX)\n",
    "to_one_hot_func = function(inputs=[_anyVector,_anyInt], outputs=to_one_hot(_anyVector, _anyInt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##defines number of objects in an object map\n",
    "class numObjects(object):\n",
    "    def __init__(self, largest=10):\n",
    "        ##npy vars\n",
    "        self.maxObjs = largest\n",
    "        ##theano expressions\n",
    "        self._K = T.scalar('numObjects',dtype='int32') ##a theano integer scalar\n",
    "        \n",
    "    def sample(self,M=1):\n",
    "        return np.random.randint(1,self.maxObjs,size=M)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.K = np.array(value,dtype=intX)\n",
    "\n",
    "##number of pixels (row, col, total) in an object map\n",
    "class numPixels(object):\n",
    "    def __init__(self, largestRows=10,largestCols=10):\n",
    "        ##npy vars\n",
    "        self.maxRows = largestRows\n",
    "        self.maxCols = largestCols\n",
    "\n",
    "        ##theano expressions\n",
    "        self._D1 = T.scalar('numPixelRows',dtype='int32') ##num pixel rows\n",
    "        self._D2 = T.scalar('numPixelCols',dtype='int32') ##num pixel cols\n",
    "        self._D = self._D1*self._D2 ##total number of pixels\n",
    "        self._D.name = 'numPixels'\n",
    "        \n",
    "    def sample(self,M=1):\n",
    "        numRows = np.random.randint(1,self.maxRows,size=M) \n",
    "        numCols = np.random.randint(1,self.maxCols,size=M)\n",
    "        numPixels = numRows*numCols\n",
    "        return numRows,numCols,numPixels\n",
    "    \n",
    "    def set_value(self,D1,D2):\n",
    "        self.D1 = np.array(D1,dtype=intX)\n",
    "        self.D2 = np.array(D2,dtype=intX)\n",
    "        self.D = D1*D2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma-distributed hyperparameter for prior distribution over object categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class priorDispersion(object):\n",
    "    def __init__(self,maxDispersion=10):\n",
    "        ##npy vars\n",
    "        self.maxDispersion = maxDispersion\n",
    "        ##theano exprs.\n",
    "        self._dispersion = T.scalar('priorDispersionParam',dtype='float32')  ##dispersion of categorical distribution\n",
    "        \n",
    "    def sample(self,M=1):\n",
    "        return np.random.gamma(self.maxDispersion,size=M)\n",
    "        \n",
    "    def set_value(self,value):\n",
    "        self.dispersion = np.array(value,dtype=floatX)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dirichlet-distributed K parameters that define multinomial object distribution prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class categoryProbs(object):\n",
    "    def __init__(self, numObjects_inst, priorDispersion_inst):\n",
    "        self.priorDispersion = priorDispersion_inst\n",
    "        self.numObjects = numObjects_inst\n",
    "        self._pi= T.matrix('categoryPrior')  ##1 x K\n",
    "        \n",
    "    def sample(self,M=1):\n",
    "        '''\n",
    "        sample(M)\n",
    "        generate M categorical distributions\n",
    "        each categorical distribution is a (1 x K) draw from a dirichlet\n",
    "        returns M x K numpy array of draws, each row an independent draw\n",
    "        '''\n",
    "        return np.random.dirichlet(np.repeat(self.priorDispersion.dispersion,self.numObjects.K),size=M)\n",
    "    \n",
    "    def set_value(self, value):\n",
    "        self.pi = np.array(value,dtype=floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object map $Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class latentObjMap(object):\n",
    "    def __init__(self,categoryPrior_inst, numPixels_inst):\n",
    "        self.categoryPrior=categoryPrior_inst\n",
    "        self.numPixels = numPixels_inst\n",
    "        \n",
    "        ##theano var for a stack of M object maps\n",
    "        self._Z = T.tensor3('Z', dtype=intX) ##(M x K x D) ##M x K x D stack of one-hot object maps\n",
    "\n",
    "        self.compile_sampler()\n",
    "        \n",
    "    def compile_sampler(self):\n",
    "        _M = T.scalar('M',dtype='int32') ##number of samples\n",
    "        _D = self.numPixels._D\n",
    "        _K = self.categoryPrior.numObjects._K\n",
    "        _pZ = T.matrix('pZ') ##(K x D) prior\n",
    "        \n",
    "        ##this will be an M x K x D int32 tensor\n",
    "        _sampleZ = rng.multinomial(pvals = repeat(_pZ.T,_M,axis=0)).reshape((_D,_M,_K)).dimshuffle((1,2,0))\n",
    "        self._Z_sample_func = function([_pZ,_M,_K,_D],outputs=_sampleZ)  \n",
    "        \n",
    "    def sample(self, M=1, pZ=None):\n",
    "        '''\n",
    "        sample(M=1)\n",
    "        returns M x K x D numpy array of samples of 1hot encoded object maps.\n",
    "        each pixel of each map is an i.i.d draw from a categorical prior\n",
    "        '''\n",
    "        K = self.categoryPrior.numObjects.K\n",
    "        D = self.numPixels.D\n",
    "        if pZ is None:\n",
    "            catProbs = self.categoryPrior.pi\n",
    "            pZ = np.repeat(catProbs.T, D, axis=1)\n",
    "        thisM = np.array(M,dtype=intX)\n",
    "        return self._Z_sample_func(pZ,thisM,K,D).astype(intX)\n",
    "    \n",
    "    def score(self,Z, given_params):\n",
    "        #so this would be p(Z|pi), i.e. the prior on Z.\n",
    "        #don't think I need this yet, so I'll pass.\n",
    "        pass\n",
    "    \n",
    "    def view_sample(self, sampleZ, show=True):\n",
    "        '''\n",
    "        view_sample(sampleZ, show=True)\n",
    "        takes a 1 x K x D sample of a 1hot encoded object map \n",
    "        and converts it to a D1 x D2 image of the map.\n",
    "        \n",
    "        makes plot if show=True\n",
    "        '''\n",
    "        sampleZImage = np.argmax(sampleZ,axis=1).reshape((self.numPixels.D1,self.numPixels.D2))\n",
    "        if show:\n",
    "            plt.imshow(sampleZImage, interpolation='nearest')\n",
    "        return sampleZImage\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 4)]\n",
      "[(3, 3), (1, 2), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "foo = [(1,2),(2,4)]\n",
    "print foo\n",
    "foo.insert(0,(3,3))\n",
    "print foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class probes(object):\n",
    "    \n",
    "    ##dumb initializer. Want to be able to process windows before assigning as attributes\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def resolve(self, shape, workingScale=None):\n",
    "        '''\n",
    "        resolve(shape, workingScale=None)\n",
    "        \n",
    "        inputs:\n",
    "            shape ~ shape tuple of the window probes (D1Prime, D2Prime)\n",
    "            workingScale ~ integer multiple of smallest possible resolution that perfectly preserves aspect ratio\n",
    "        \n",
    "        outputs:\n",
    "            resolutions ~ list of shape tuples. all possible downsamples that perfectly preserve aspect ratio\n",
    "            workingResolution ~ shape tuple of the working scale used for the windows. pass along with windows to \"reshape\".\n",
    "            \n",
    "        You want a working scale that is small as possible without sacrificing any of the objects\n",
    "        in the target image and preserves number of objects in smallest window and number of objects in most dense window.\n",
    "\n",
    "        '''\n",
    "     \n",
    "        ##get the smallest permissable downsample\n",
    "        greatestCommonDivisor = gcd(shape[0], shape[1])\n",
    "\n",
    "        ##generate all permissable resolutions up to max (i.e. all integer rescalings)\n",
    "        smallestResolution = (shape[0]/greatestCommonDivisor, shape[1]/greatestCommonDivisor)\n",
    "        resolutions = [(ii*smallestResolution[0], ii*smallestResolution[1]) for ii in range(2,greatestCommonDivisor+1,2)]\n",
    "        resolutions.insert(0,smallestResolution)\n",
    "        ##get the working window resolution\n",
    "        if workingScale is None:\n",
    "            workingScale = -1\n",
    "        \n",
    "        workingResolution = resolutions[workingScale]\n",
    "        \n",
    "        return resolutions, workingResolution\n",
    "    \n",
    "    def reshape(self, windows, reshapeTo):\n",
    "        \n",
    "        ##downsample factor\n",
    "        N = windows.shape[0]\n",
    "        \n",
    "        dns = np.prod(reshapeTo)/np.prod(windows.shape[-2])\n",
    "        \n",
    "        ##check that smallest window is greater than one pixel\n",
    "        windowSizes = np.sum(np.sum(windows,axis=2),axis=1)\n",
    "        \n",
    "        smallestWindowSize = np.min(windowSizes)\n",
    "        sizeOfThisWindowAfterDownsampling = smallestWindowSize*dns\n",
    "        if sizeOfThisWindowAfterDownsampling < 1:\n",
    "            raise ValueError('too much downsampling. smallest window less than one pixel')\n",
    "        else:\n",
    "            windows = np.array(map(lambda x: imresize(x,reshapeTo,interp='nearest', mode='L'), windows))\n",
    "            \n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def set_value(self,windows,flatten = False):\n",
    "        self.N,self.D1Prime,self.D2Prime = windows.shape\n",
    "        self.DPrime = self.D1Prime*self.D2Prime\n",
    "        if flatten:\n",
    "            windows = windows.reshape((self.N,self.DPrime))\n",
    "        self.windows = windows.astype(intX)\n",
    "\n",
    "         \n",
    "        \n",
    "    \n",
    "    def make_windows(self, makeWindows=False, stride = 1, sizes = [1], n_groups=7, group_order = 2):\n",
    "        if makeWindows:\n",
    "            Windows = []\n",
    "            for sz in sizes:\n",
    "                scale_count = 0\n",
    "                for rows in np.arange(sz,D1,stride,dtype=int, ):\n",
    "                    for cols in np.arange(sz,D2,stride,dtype=int):\n",
    "                        one_win = np.zeros((D1,D2),dtype=floatX)\n",
    "                        one_win[(rows-sz):(rows+sz), (cols-sz):(cols+sz)]=1\n",
    "                        Windows.append(one_win)\n",
    "                        scale_count +=1\n",
    "                print scale_count\n",
    "\n",
    "\n",
    "            N = len(Windows)\n",
    "            n_groups *= D \n",
    "            W = np.zeros((N+n_groups,D),dtype=intX)\n",
    "            for n in range(N):\n",
    "                W[n,:] = Windows.pop().ravel()\n",
    "\n",
    "            for n in range(N,N+n_groups):\n",
    "                rand_pairs = np.random.permutation(N)[:group_order]\n",
    "                W[n,:] = np.clip(np.sum(W[rand_pairs[0:group_order],:],axis=0), 0, 1)\n",
    "\n",
    "            W = W[:(N+n_groups),:]\n",
    "\n",
    "\n",
    "            N = W.shape[0]\n",
    "            return W.astype(intX)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class target_image(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_values(self,targetObjectMap, targetImage=None):\n",
    "        self.targetImage = targetImage\n",
    "        values = np.array(np.unique(targetObjectMap))\n",
    "        self.targetObjectMap = np.digitize(np.array(targetObjectMap), bins=values, right=True ).astype(intX)\n",
    "        values = np.array(np.unique(targetObjectMap))\n",
    "        self.numberTargetObjects = len(values)\n",
    "        ##one-hot encoding\n",
    "        D = np.prod(self.targetObjectMap.shape)\n",
    "        K = self.numberTargetObjects\n",
    "        self.testObjectMapOneHot = np.eye(K)[self.targetObjectMap.ravel()].T.reshape((1,K,D))\n",
    "\n",
    "        \n",
    "    def reshape(self,targetObjectMap, reshapeTo, targetImage=None):\n",
    "        values = np.array(np.unique(targetObjectMap))\n",
    "        imK = len(values)\n",
    "        \n",
    "\n",
    "        ##resize to window, checking for preserved number of objects\n",
    "        testObjectMap=imresize(targetObjectMap, reshapeTo, interp='nearest')\n",
    "        values = np.array(np.unique(testObjectMap))\n",
    "        assert imK==len(values)\n",
    "\n",
    "        ##digitize, checking \n",
    "        testObjectMap=np.digitize(np.array(testObjectMap), bins=values, right=True ).astype(int)\n",
    "        values = np.array(np.unique(testObjectMap))\n",
    "        assert imK==len(values)\n",
    "        \n",
    "        if targetImage is not None:\n",
    "            testTargetImage = imresize(targetImage, reshapeTo, interp='nearest')\n",
    "            return testObjectMap, testTargetImage\n",
    "        else:\n",
    "            return testObjectMap\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise params $\\theta_{+}, \\theta_{-}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class noiseParams(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def enumerate_param_grid(self,theta_dns):\n",
    "        '''\n",
    "        enumerate_param_grid(theta_dns)\n",
    "        inputs:\n",
    "        theta_dns ~ integer indicating roughly sqrt(G)*2, G = number of grid points\n",
    "        outputs:\n",
    "        noise_param ~ 2 x G np array. columns are pairs of noise params, [p_on; p_off]. p_off < p_on for each pair.\n",
    "        '''\n",
    "        p_on, p_off = noise_grid(theta_dns,theta_dns)\n",
    "        return p_on, p_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed responses $\\mathbf{r}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class responses(object):\n",
    "    '''\n",
    "    responses(latentObjMap_inst, noiseParams_inst)\n",
    "    inputs:\n",
    "        latentObjMap_inst, noiseParams_inst ~ instances of (see above)\n",
    "    \n",
    "    outputs:\n",
    "        construct a responses instance with main attribute \"lkhd_cube\" that \n",
    "        allows to compute likelihood of responses given an object map Z\n",
    "        \n",
    "        can set observed responses to known windows using 'set_values'\n",
    "    '''\n",
    "    def __init__(self,latentObjMap_inst, noiseParams_inst):\n",
    "        self.Z = latentObjMap_inst\n",
    "        self.noise = noiseParams_inst\n",
    "        \n",
    "        ##a G x K x K cube of p(r|count,p_on,p_off)\n",
    "        ##K = max count and/or response, G = number of noise params \n",
    "        self.lkhd_cube = T.tensor3('lkhd_cube')  \n",
    "        \n",
    "#         self.windows=windows ##N x D', where D' = number of stimulus pixels.\n",
    "       \n",
    "        \n",
    "        \n",
    "        self._W = T.matrix('windows', dtype=intX)\n",
    "        self._DPrime = self._W.shape[1]\n",
    "        \n",
    "        self.compile_upsampler()\n",
    "        self.compile_object_counter()\n",
    "        \n",
    "    def compile_upsampler(self):\n",
    "        ##get theano vars for Z and it's dimensions\n",
    "        _Z = self.Z._Z ##M x K x D stack of one-hot object maps\n",
    "        _M = _Z.shape[0]\n",
    "        _K = self.Z.categoryPrior.numObjects._K\n",
    "        _D1 = self.Z.numPixels._D1\n",
    "        _D2 = self.Z.numPixels._D2\n",
    "        _D = self.Z.numPixels._D\n",
    "        \n",
    "        ##compute an upscale factor so we can upsample pixels in Z to match W\n",
    "        _upscale = T.sqrt(self._DPrime // _D).astype(intX)\n",
    "        \n",
    "        ##this converts back to image format (M x D1 x D2), where D = D1*D2. int32\n",
    "        _Z_images = _Z.argmax(axis=1).reshape((_M,_D1,_D2), ndim=3) \n",
    "\n",
    "        ##this upsamples the Z-maps to (M x D1' x D2'), where D' = D1'*D2'. still int32\n",
    "        _Z_upsamples = T.tile(_Z_images.dimshuffle(0,1,2,'x','x'), (_upscale,_upscale)).transpose(0,1,3,2,4).reshape((_M,_D1*_upscale, _D2*_upscale))\n",
    "        \n",
    "        ##make a function so we can see upsampled object maps as images (if needed)\n",
    "        self._Z_usample_image_func = function([_M, _D1, _D2, self._DPrime, _Z], outputs=_Z_upsamples)\n",
    "        \n",
    "        ##this converts the Z-maps back to a one-hot encoding (M x K x D'). \n",
    "        _Z_upsamples_one_hot = to_one_hot(_Z_upsamples.ravel(), _K,dtype=intX).reshape((_M, self._DPrime, _K)).transpose((0,2,1))\n",
    "        self._Z_upsample_one_hot_func = function([_M, _K, _D1, _D2, self._DPrime, _Z], outputs=_Z_upsamples_one_hot)\n",
    "    \n",
    "    def compile_object_counter(self):\n",
    "\n",
    "        _Z = self.Z._Z ##M x K x D stack of one-hot object maps\n",
    "        _W = self._W\n",
    "       \n",
    "        ##(M x K x 1 x D')\n",
    "        ##         N x D'\n",
    "        ##(M x K x N x D')  sum(D')\n",
    "        ##(M x K x N)      clip(0,1)\n",
    "        ##(M x K x N)      sum(K)\n",
    "        ##(M x N)\n",
    "        self._object_counts = T.sum(_Z.dimshuffle((0,1,'x',2))*_W,axis=-1).clip(0,1).sum(axis=1)\n",
    "        self._object_count_func = function([_Z, _W], outputs=self._object_counts)\n",
    "\n",
    "    \n",
    "    def compute_feature(self,Z, winIdx=None):\n",
    "        '''\n",
    "        compute_feature(Z, winIdx = None)\n",
    "        the feature in this case is an object count over a map Z given a window W\n",
    "        \n",
    "        inputs:\n",
    "            Z is an MxKxD stack of one-hot encoded object maps\n",
    "            winIdx ~ array of window indices for getting responses.\n",
    "                     if supplied, N' = len(winIdx), otherwise N' = total number of windows\n",
    "        \n",
    "        output:\n",
    "            applies N'xD' matrix of windows to map Z, returns MxN' matrix of object counts\n",
    "            note: if D != D', upsamples Z's so they each have D' pixels\n",
    "        '''\n",
    "        M = np.array(Z.shape[0],dtype=intX)\n",
    "        K = self.Z.categoryPrior.numObjects.K\n",
    "        D1 = self.Z.numPixels.D1\n",
    "        D2 = self.Z.numPixels.D2\n",
    "        D = self.Z.numPixels.D\n",
    "        if winIdx is not None:\n",
    "            windows = self.windows[winIdx]\n",
    "        else:\n",
    "            windows = self.windows\n",
    "        if D != self.DPrime:\n",
    "#             print 'upsampling'\n",
    "            Z = self._Z_upsample_one_hot_func(M,K,D1,D2,self.DPrime,Z)\n",
    "            \n",
    "        return self._object_count_func(Z,windows)\n",
    "    \n",
    "    def score(self,r,c,p_on,p_off):\n",
    "        '''\n",
    "        calculates p(response | object_count, p_on, p_off), the strange likelihood I derived for this model\n",
    "        \n",
    "        score(response,object_count,p_on,p_off)\n",
    "        inputs:\n",
    "            response ~ integer\n",
    "            object_count ~ integer\n",
    "            p_on,p_off ~ noise params\n",
    "        outputs:\n",
    "            scalar likelihood value. \n",
    "        '''\n",
    "        \n",
    "        ##\n",
    "        K = self.Z.categoryPrior.numObjects.K\n",
    "        try:\n",
    "            counts = np.array([nCk(c,m)*nCk(K-c, r-m) for m in range(min(r,c)+1)])\n",
    "            probs = np.array([(1-p_on)**(c-m) * (p_on)**m * (p_off)**(r-m) * (1-p_off)**(K-c-r+m) for m in range(min(r,c)+1)])\n",
    "            return counts.dot(probs)\n",
    "        except TypeError:\n",
    "            print 'Error in score. response and count should be integer-valued.'\n",
    "            print r\n",
    "            print c\n",
    "            print min(r,c)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def make_lkhd_cube(self,p_on, p_off):\n",
    "        '''\n",
    "        make_lkhd_cube(p_on,p_off)\n",
    "        inputs:\n",
    "            p_on,p_off ~ two array-likes of matching length=G giving pairs of noise params\n",
    "        outputs:\n",
    "            lkhd_cube ~ G x (K+1) x K tensor of \"scores\". dim1 = noise params, dim2 = responses, dim3 = conditioning counts\n",
    "            The response dimension has size K+1 because subjects can respond 0 through K.\n",
    "        \n",
    "        '''\n",
    "        K = self.Z.categoryPrior.numObjects.K\n",
    "        countRange = np.arange(1,K+1,dtype=intX)\n",
    "        respRange = np.arange(0,K+1,dtype=intX)\n",
    "        G = len(p_on)\n",
    "        lkhd_cube = np.full((G,K+1,K),0,dtype=floatX)\n",
    "        for g,p in enumerate(zip(p_on,p_off)):\n",
    "            for r in respRange:\n",
    "                for c in countRange:\n",
    "                    lkhd_cube[g,r,c-1]  = self.score(r,c,p[0],p[1])\n",
    "        return lkhd_cube\n",
    "\n",
    "        \n",
    "    def sample(self,Z,p_on,p_off):\n",
    "        \n",
    "        '''\n",
    "        sample(Z,p_on,p_off)\n",
    "        inputs:\n",
    "            Z   ~  M x K x D\n",
    "            p_on,p_off ~ noise params\n",
    "\n",
    "        returns\n",
    "         noisy_object_counts  = M x N matrix of counts drawn i.i.d from likelihood\n",
    "\n",
    "        this is not part of VI, so this is all done on cpu (no theano)\n",
    "        '''\n",
    "        M = Z.shape[0]  ##number of maps to sample\n",
    "        K = self.Z.categoryPrior.numObjects.K\n",
    "        object_counts = self.compute_feature(Z)\n",
    "        N = object_counts.shape[1]\n",
    "        sample_responses = np.zeros((M,N), dtype = intX)\n",
    "        for m in range(M):\n",
    "            for n in range(N):\n",
    "                resp_dist = np.zeros(K+1)\n",
    "                oc = np.int(object_counts[m,n])\n",
    "                for k in range(K+1):\n",
    "                    resp_dist[k] = self.score(k,oc,p_on,p_off)\n",
    "                sample_responses[m,n]=np.argmax(np.random.multinomial(1,resp_dist))\n",
    "        return sample_responses\n",
    "    \n",
    "    def set_values(self,data=None, windows=None):\n",
    "        '''\n",
    "        set_values(data, windows)\n",
    "        inputs: supply either or both\n",
    "            data ~ (1 x N) or (N x 1) or (N,) array of integer responses\n",
    "            windows ~ instance of \"probes\" class containing (N x D') array of binary probes, where D' = = number of stimulus pixels.\n",
    "        outputs:\n",
    "            sets the observations attribute to data\n",
    "            sets the windows attributes to windows\n",
    "            \n",
    "            checks for basic compatibility between windows and responses\n",
    "        '''\n",
    "        if data is not None:\n",
    "            self.observations = np.squeeze(data) ##array of shape (N,)\n",
    "        if windows is not None:\n",
    "            \n",
    "            self.windows = windows.windows ##N x D', where D' = number of stimulus pixels.\n",
    "            self.N = self.windows.shape[0]\n",
    "            self.DPrime = np.array(windows.windows.shape[1],dtype=intX)\n",
    "            self.D1Prime = windows.D1Prime\n",
    "            self.D2Prime = windows.D2Prime\n",
    "            if self.DPrime != self.D1Prime*self.D2Prime:\n",
    "                raise ValueError('aspect ratio of windows is not right')\n",
    "        if hasattr(self,'observations') & hasattr(self, 'windows'):\n",
    "            Nobs = self.observations.shape[0]\n",
    "            if  Nobs != self.N:\n",
    "                raise ValueError(\"number of windows (%d) and data points (%d) don't match\" %(self.N, Nobs))\n",
    "            if any(map(lambda x,y: x>y, self.observations, self.windows.sum(axis=1))):\n",
    "                raise ValueError('some responses are larger than number of pixels in corresponding window. probably over-downsampled windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variational updates and optimization procedures for model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class inferQZ(object):\n",
    "    def __init__(self):\n",
    "        self.compile_updater()\n",
    "        \n",
    "    def compile_updater(self):\n",
    "        ##K x N x K tensor of object count probs\n",
    "        ##the first K is object id, the last K is object count\n",
    "        _oc_probs = T.tensor3('oc_probs') \n",
    "\n",
    "        ##N x K, this will just be the log of P_star. \n",
    "        _lnP_star = T.matrix('lnP_star')\n",
    "\n",
    "        ## K x 1, this is place holder for the dot product between pixel value and expected log of hyperparameters pi\n",
    "        _v = T.matrix('prior_penalties')\n",
    "\n",
    "              ##K x N x K oc_probs\n",
    "        ##(tensordot)\n",
    "                  ##N x K lnP_star \n",
    "              ##K x 1     lnQ_z\n",
    "        ##(add V)\n",
    "              ##K x 1\n",
    "        ##(exp)\n",
    "              ##K x 1     Q_z_nn\n",
    "        ##(normalize)\n",
    "              ##K x 1     Q_z, the variational posterior element for one pixel\n",
    "\n",
    "        ##log of Q_z (minus unknown constant that normalizes things)    \n",
    "        _lnQ_z = T.tensordot(_oc_probs, _lnP_star, [[1,2], [0,1]]).dimshuffle([0,'x'])+_v\n",
    "\n",
    "        ##non-normalized Q_z. we shift by max value to stabilize upcoming exp\n",
    "        _Q_z_nn = T.exp(_lnQ_z-T.max(_lnQ_z)) \n",
    "\n",
    "        ##variational posterior prob for one pixel\n",
    "        _Q_z = _Q_z_nn / _Q_z_nn.sum()\n",
    "        \n",
    "        self._update_func = function([_oc_probs, _lnP_star, _v], outputs = _Q_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class optimizeNoiseParams(object):\n",
    "    def __init__(self):\n",
    "        'hi'\n",
    "        \n",
    "        \n",
    "    def update_noiseParams(self, goodnessOfFit):\n",
    "        '''\n",
    "        update_noiseParams(goodnessOfFit)\n",
    "        \n",
    "        goodnessOfFit ~ G x 1 array of g.o.f. measures, one for each discrete candidate of noise params.\n",
    "        returns max, argmax of this array\n",
    "        \n",
    "        '''\n",
    "        ##find index of the best lkhd params (scalar integer)\n",
    "        goodnessOfFitStar = np.max(goodnessOfFit)\n",
    "        noiseParamStarIdx = np.argmax(goodnessOfFit)\n",
    "        return goodnessOfFitStar, noiseParamStarIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class inferQPi(object):\n",
    "    def __init__(self,):\n",
    "        self.compile_updater()\n",
    "        \n",
    "    def compile_updater(self):\n",
    "        _alpha_0 = T.scalar('alpha_0')\n",
    "        _q_Z = T.matrix('q_Z')  ##K x 1, this is result of summing over pixels in Q_Z matrix. very different from Q_z\n",
    "\n",
    "        _alpha = _q_Z + _alpha_0 ##broadcasts the scalar _alpha_0 across K\n",
    "\n",
    "        ##(K x 1) this is needed to update Q_z\n",
    "        _Eln_pi = T.psi(_alpha) - T.psi(_alpha.sum())\n",
    "\n",
    "        ##(K x 1) not needed for update of variational posteriors, but we'll want it for model interpretation\n",
    "        _E_pi = _alpha / _alpha.sum()\n",
    "        \n",
    "        ##returns _Eln_pi (K x 1) (object ids x 1)\n",
    "        ##returns _E_pi (K x 1) (object ids x 1)\n",
    "        self._update_func = function([_q_Z, _alpha_0], outputs = [_Eln_pi, _E_pi])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational inference for $q(Z)$, $q(\\pi)$, noiseparams\n",
    "\n",
    "*Nasty book-keeping note*: counts, responses, and object id's are often converted to indices, or to 1hot formats.\n",
    "To convert counts to indices we need to subtract 1, because counts always range from 1 to K (inclusive),\n",
    "whereas indices range from 0 to K-1.\n",
    "\n",
    "To convert object id's to indices we don't need to subtract 1, because id's are coded from 0 to (K-1).\n",
    "\n",
    "To convert responses to indices we don't need to subtract, because responses range from 0 to K (inclusive).\n",
    "Becuase 0 to K (inclusive) is K+1 numbers, we do need to make sure that arrays for storing discrete responses have K+1 elements.\n",
    "\n",
    "Whew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VI(object):\n",
    "    def __init__(self, responses_inst, inferQZ_inst, optimizeNoiseParams_inst, inferQPi_inst):\n",
    "        self.responses = responses_inst\n",
    "        self.iQZ = inferQZ_inst\n",
    "        self.oNP = optimizeNoiseParams_inst\n",
    "        self.iQPi = inferQPi_inst\n",
    "        \n",
    "        ##compile theano expressions, functions\n",
    "        self.compile_object_probs()\n",
    "        self.compile_ELBO()\n",
    "        self.compile_predictive_distribution()\n",
    "        self.compile_goodness_of_fit()\n",
    "        \n",
    "        \n",
    "\n",
    "    ##===compile theano expressions, functions===\n",
    "    def compile_object_probs(self):\n",
    "        _object_counts = self.responses._object_counts ##(M x N)\n",
    "        _K = self.responses.Z.categoryPrior.numObjects._K\n",
    "        \n",
    "        ##non-normalized object count probs (N x K)\n",
    "        ##we are using these counts as indices, so we have to subtract 1\n",
    "        _object_count_prob_nn = to_one_hot(_object_counts.astype('int32').flatten()-1,_K).reshape((_object_counts.shape[0],_object_counts.shape[1],_K)).sum(axis=0)\n",
    "\n",
    "        ##object count probs (N x K)\n",
    "        _object_count_prob = _object_count_prob_nn / _object_count_prob_nn.sum(axis=1).reshape((_object_count_prob_nn.shape[0], 1))\n",
    "        \n",
    "        self.object_count_prob_func = function([_object_counts, _K], outputs = _object_count_prob)\n",
    "        \n",
    "    def compile_goodness_of_fit(self):\n",
    "        _P_theta = T.tensor3('_P_theta') ##(G x N x K)\n",
    "        _oc_probs = T.matrix('oc_probs') ##N x K ~ this is a place holder for the \"object count prob\" matrix\n",
    "\n",
    "\n",
    "        ##(G x N x K)\n",
    "        ##(    N x K)  (dot product, broadcast across G)\n",
    "        ##(G x 1)  --> because we don't do vectors we reshape to make output 2Dimensional (G x 1)\n",
    "\n",
    "        ##log of likelihood\n",
    "\n",
    "        _ln_P_theta = T.log(_P_theta)\n",
    "\n",
    "        ##ln [q(theta+, theta-) - const (G x 1) ], the log of the unormalized variational distribution over theta\n",
    "        ##currently we're taking a point estimate on theta so we don't do the hard normalization step.\n",
    "        _goodnessOfFit = T.tensordot(_ln_P_theta, _oc_probs, axes=[[1,2], [0,1]],).reshape((_P_theta.shape[0], 1))\n",
    "        \n",
    "        ##returns G x 1 goodness of fit array. each element gives goodness of fit for one possible value of noiseparams\n",
    "        self.goodness_of_fit_func = function([_P_theta, _oc_probs], outputs = _goodnessOfFit)\n",
    "        \n",
    "     \n",
    "    def compile_ELBO(self):\n",
    "        _goodnessOfFitStar = T.scalar('_goodnessOfFitStar') ##scalar\n",
    "\n",
    "        _qZ = T.matrix('qZ_holder') ##N x K \n",
    "\n",
    "        ##scalar: the entropy of the variational posterior\n",
    "        a_min = 10e-15\n",
    "        a_max = 1\n",
    "        _posterior_entropy = -T.tensordot(_qZ.clip(a_min,a_max), T.log(_qZ.clip(a_min,a_max)))\n",
    "\n",
    "        ##scalar\n",
    "        _ELBO = _goodnessOfFitStar  + _posterior_entropy\n",
    "        self.ELBO_update_func = function([_qZ, _goodnessOfFitStar], outputs=[_goodnessOfFitStar, _posterior_entropy, _ELBO])\n",
    "\n",
    "    def compile_predictive_distribution(self):\n",
    "        _lkhdTable = T.matrix('lkhd_table_pred') ##K+1 x K ~ responses x counts, likelihood for fixed noiseparams\n",
    "        \n",
    "        ##windows x 1         x counts  |\n",
    "        ##          responses x counts    tensordot\n",
    "        ##windows x responses\n",
    "        _oc_probs = T.matrix('oc_probs') ##N x K ~ this is a place holder for the \"object count prob\" matrix\n",
    "        _pred_dist = T.tensordot(_oc_probs, _lkhdTable, axes = [[1],[1]])\n",
    "        self.predictive_distribution_update_func = function(inputs=[_oc_probs,_lkhdTable], outputs=_pred_dist)\n",
    "       \n",
    "        \n",
    "    ##====initialization methods\n",
    "    \n",
    "    def init_number_of_objects_range(self, numOverMin=1):\n",
    "        \n",
    "        ##the smallest number of objects has to be max response to any one probe\n",
    "        smallestPossibleNumberOfObjects = np.max(self.curResponses).astype(intX)\n",
    "        self.numberOfObjectsRange = np.arange(smallestPossibleNumberOfObjects, smallestPossibleNumberOfObjects+numOverMin+1)\n",
    "        \n",
    "        ##----there's also this complicated thing you could do...\n",
    "#         maxResponseIdx = np.argmax(self.curResponses)\n",
    "        \n",
    "#         ##we'll cover a range from this minimum up to a number determined by probe sizes\n",
    "#         sizeOfMaxResponseWindow = np.sum(self.responses.windows[self.curIdx[maxResponseIdx], :])\n",
    "#         sizeOfLargestWindow = np.max(np.sum(self.responses.windows, axis=1))\n",
    "#         differenceInWindowSizes = sizeOfLargestWindow - sizeOfMaxResponseWindow\n",
    "        \n",
    "#         largestNumberOfObjects = np.rint(np.min(smallestPossibleNumberOfObjects + hyperHyper*differenceInWindowSizes, self.hardcodedMaxNumberOfObjects)).astype(intX)\n",
    "#         self.number_of_objects_range = np.arange(smallestPossibleNumberOfObjects, largestNumberOfObjects)\n",
    "        \n",
    "    def init_pixel_resolution_range(self, numOverMin = 1):\n",
    "        \n",
    "        D1Prime,D2Prime = self.responses.D1Prime, self.responses.D2Prime\n",
    "        resolutions,_ = probes().resolve((D1Prime, D2Prime))\n",
    "        self.pixelResolutionRange = resolutions[:numOverMin]\n",
    "        \n",
    "#         if not hasattr(self, 'pixel_resolution_range'):\n",
    "#             if listOfTuples is None:\n",
    "#                 raise ValueError('you gotta initialize pixel resolution range by hand. run init_pixel_resoltuion and supply list of shape tuples')\n",
    "#             else:\n",
    "#                 self.pixel_resolution_range = listOfTuples                \n",
    "#         else:\n",
    "#             print 'pixel resoltuion range already intialized by hand'\n",
    "            \n",
    " \n",
    "        \n",
    "        \n",
    "    \n",
    "    ##select initial values of noiseparams, discretize, access lkhdCube\n",
    "    def init_noiseParams(self, pOnInit, pOffInit,noiseParamNumber):\n",
    "        '''\n",
    "        init_noiseParams(pOnInit, pOffInit,noiseParamNumber)\n",
    "        \n",
    "        create a grid of candidate noise params according noiseParamNumber (i.e., something like square root of number\n",
    "        of candidates we'll consider)\n",
    "        \n",
    "        take initial guess at noise param and return index of nearest candidate in the grid\n",
    "        \n",
    "        '''\n",
    "\n",
    "        ##will need the likelihood cube at whatever resolution we're using to infer noise params, so make here\n",
    "        self.noiseParamGrid = np.array(self.responses.noise.enumerate_param_grid(noiseParamNumber),dtype=floatX).T\n",
    "        \n",
    "        ##for starters, find index of closest noise param values to pOnInit, pOffInit, and then slice the lkhdCube\n",
    "        noiseParamIdx = np.argmin(map(np.linalg.norm, self.noiseParamGrid-np.array([pOnInit,pOffInit]).T))\n",
    "        return noiseParamIdx                                      \n",
    "       \n",
    "    \n",
    "    def init_Eln_pi(self, qZ):\n",
    "        dirichletParameter = self.responses.Z.categoryPrior.priorDispersion.dispersion\n",
    "        Eln_pi, qPi = self.iQPi._update_func(qZ.sum(axis=1, keepdims=True), dirichletParameter)\n",
    "        return Eln_pi, qPi\n",
    "    \n",
    "    def init_qZ(self, numStarterMaps, empiricalLkhdTableStar, dirichletParameter):\n",
    "        '''\n",
    "        qZ, startZ, starerLogs = init_q_Z(numStarterMaps, empiricalLkhdTable, dirichletParameter)\n",
    "        \n",
    "        randomly generates a stack of potential latent object maps.\n",
    "        scores each Z according to p(responses | Z)\n",
    "        selects Z with highest score = Z*\n",
    "        constructs an initial qZ by sampling from dirichlet(Z[:,d]*+dirichletParameter) for each pixel d.\n",
    "        \n",
    "        inputs:\n",
    "            numStartermaps ~ number of randomly generated Z's\n",
    "            empiricalLkhdTable ~ N x K matrix of likelihoods p(response | count, pOn, pOff).\n",
    "                                 noise params are implicit and assumed to be best guess at start of running vi.\n",
    "                                 N can be either Ntrn, Ntest, or Nreg\n",
    "            dirichletParameter ~ slop that you add to best Z to make a qZ\n",
    "            \n",
    "        outputs:\n",
    "            qZ ~ K x D variational posterior\n",
    "            startZ ~ the best object map from the randomly generated stack\n",
    "            starterLogs ~ the l\n",
    "        '''\n",
    "\n",
    "        ##for generating stacks of one-hot-encoded object maps. these are \"special\" smooth maps\n",
    "        def make_object_map_stack(K, num_rows, num_cols, image_dimensions,num_maps):\n",
    "            size_of_field = int(np.mean(image_dimensions))\n",
    "            D = np.prod(image_dimensions)\n",
    "            object_map_base = spm(num_rows,num_cols,size_of_field,cluster_pref = 'random',number_of_clusters = K)\n",
    "            object_maps = np.zeros((num_maps, K, D),dtype=intX)\n",
    "            for nm in range(num_maps):\n",
    "                object_map_base.scatter()\n",
    "                tmp = np.squeeze(object_map_base.nn_interpolation())\n",
    "                tmp = imresize(tmp, image_dimensions, interp='nearest')\n",
    "                ##convert to one_hot encoding\n",
    "                tmp = np.eye(K)[tmp.ravel()-1].T  ##K x D\n",
    "                object_maps[nm] = tmp\n",
    "            return object_maps\n",
    "\n",
    "        ##generate candidate maps for starting iQ_Z\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        D1 = self.responses.Z.numPixels.D1\n",
    "        D2 = self.responses.Z.numPixels.D2\n",
    "        D = D1*D2\n",
    "        objectMapStack = make_object_map_stack(K, 2*K, 2*K, (D1,D2), numStarterMaps)\n",
    "\n",
    "        ##get table of log likelihoods for observed data\n",
    "        logEmpiricalLkhdTable = np.log(empiricalLkhdTableStar)\n",
    "  \n",
    "        ##get object_counts ( M x N)\n",
    "        objectCounts = self.responses.compute_feature(objectMapStack,winIdx=self.curIdx)\n",
    "\n",
    "        ##get one-hot encoding of object counts (M x N x K)\n",
    "        ##Use functionalized theano version\n",
    "        ##we are using these counts as indices, so we have to subtract 1\n",
    "        oneHotObjectCounts = to_one_hot_func(objectCounts.astype(intX).flatten()-1, K).reshape((objectCounts.shape[0],objectCounts.shape[1],K))\n",
    "        \n",
    "\n",
    "\n",
    "        ##evaluate log_likelihoods\n",
    "        ## one_hot_object_counts = M x N x K\n",
    "        ##            lkhd_table =     N x K tensordot\n",
    "        ## observedLogLkhds = M x 1\n",
    "        starterLogs = np.tensordot(oneHotObjectCounts, logEmpiricalLkhdTable, axes=2)\n",
    "        bestStartMap = np.argmax(starterLogs)\n",
    "        startZ = objectMapStack[bestStartMap] ## K x D\n",
    "        \n",
    "        qZ = np.zeros((K,D), dtype=floatX)\n",
    "        for d in range(D): \n",
    "            qZ[:,d] = np.random.dirichlet(startZ[:,d]+dirichletParameter)\n",
    "        return qZ, startZ, starterLogs\n",
    "   \n",
    "\n",
    "    ##==========update and optimization methods=============\n",
    "    \n",
    "    def optimize_hyper_parameters(self):\n",
    "        bestPercentCorrect = 0\n",
    "        bestK = np.inf\n",
    "        bestD = np.inf\n",
    "        for model in self.storedModels.values():\n",
    "            if model.bestPercentCorrect > bestPercentCorrect:\n",
    "                bestPercentCorrect = model.bestPercentCorrect\n",
    "                bestModel = model\n",
    "                bestK = model.responses.Z.categoryPrior.numObjects.K\n",
    "                bestD = model.responses.Z.numPixels.D\n",
    "            elif model.bestPercentCorrect == bestPercentCorrect:\n",
    "                if (model.responses.Z.categoryPrior.numObjects.K < bestK) or (model.responses.Z.numPixels.D < bestD):\n",
    "                    bestPercentCorrect = model.bestPercentCorrect\n",
    "                    bestModel = model\n",
    "                    bestK = model.responses.Z.categoryPrior.numObjects.K\n",
    "                    bestD = model.responses.Z.numPixels.D\n",
    "        return bestModel\n",
    "            \n",
    "    \n",
    "    def update_qZ(self, qZ, ExpLnPi, noiseParamStarIdx):\n",
    "        '''\n",
    "        coordinate ascent on variational posteriors of object map pixels\n",
    "        \n",
    "        update_qZ(qZ, ExpLnPi, noiseParamStarIdx)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        ##PStar is our term for the empirical lkhd cube sliced at noiseParamStarIdx. it is N x K\n",
    "        logPStar = np.log(self.curEmpiricalLkhdCube[noiseParamStarIdx])\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        D = self.responses.Z.numPixels.D\n",
    "        for d in range(D):\n",
    "            sampledZ = self.responses.Z.sample(M=self.numSamples, pZ=qZ)\n",
    "            oc_probs = np.zeros((K, self.curN, K),dtype=floatX)\n",
    "            v = np.zeros((K,1), dtype=floatX)\n",
    "            for k in range(K): \n",
    "                sampledZ[:,:,d] = 0.    ##clear out object assignment for pixel d\n",
    "                sampledZ[:,k, d] = 1.   ##assign pixel d to object k\n",
    "                oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "                oc_probs[k,:,:] = self.object_count_prob_func(oc_counts,K) ##calculate object count probs. given this assignment\n",
    "                v[k] = np.dot(sampledZ[0,:,d], ExpLnPi,)        ##compare current assignment to prior over assignments\n",
    "            qZ[:, d] = self.iQZ._update_func(oc_probs, logPStar, v).squeeze() ##update variational posterior for pixel d\n",
    "        return qZ   \n",
    "\n",
    "    def update_goodness_of_fit(self, qZ):\n",
    "        '''\n",
    "        update_goodness_of_fit(qZ)\n",
    "        \n",
    "        return G x 1 array of measures of how well the variational posterior qZ matches the data\n",
    "        for each of the G possible values of the noise params.\n",
    "        \n",
    "        '''\n",
    "        empiricalLkhdCube = self.curEmpiricalLkhdCube\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        \n",
    "        sampledZ = self.responses.Z.sample(M=self.numSamples, pZ=qZ)\n",
    "        oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "        oc_probs = self.object_count_prob_func(oc_counts, K)\n",
    "        \n",
    "        goodnessOfFit = self.goodness_of_fit_func(empiricalLkhdCube, oc_probs)\n",
    "        \n",
    "        return goodnessOfFit\n",
    "            \n",
    "                                    \n",
    "    def optimize_PStar(self,qZ):\n",
    "        '''\n",
    "        optimize_PStar(qZ)\n",
    "        \n",
    "        PStar is what we call the empirical lkhd cube sliced at the current best noise params.\n",
    "        So, it is our current guess at the empricial lkhd table.\n",
    "        \n",
    "        We find the best noise param (by measuring goodness of fit)\n",
    "        '''\n",
    "        goodnessOfFit = self.update_goodness_of_fit(qZ)\n",
    "        goodnessOfFitStar, noiseParamStarIdx = self.oNP.update_noiseParams(goodnessOfFit)\n",
    "\n",
    "        ##N x K \n",
    "        PStar = self.curEmpiricalLkhdCube[noiseParamStarIdx]\n",
    "        \n",
    "        #1 x 2 best noiseparams\n",
    "        noiseParamStar = self.noiseParamGrid[noiseParamStarIdx,:]\n",
    "        return noiseParamStar, noiseParamStarIdx, PStar, goodnessOfFitStar\n",
    "    \n",
    "    def update_qPi(self, qZ):\n",
    "        ##update prior params\n",
    "        dirchletParam=self.responses.Z.categoryPrior.priorDispersion.dispersion\n",
    "        ExpLnPi, qPi = self.iQPi._update_func(qZ.sum(axis=1, keepdims=True),dirchletParam)\n",
    "        return ExpLnPi, qPi\n",
    "    \n",
    "\n",
    "        \n",
    "    ##===criticism===        \n",
    "    def update_ELBO(self, qZ, goodnessOfFitStar):\n",
    "        goodnessOfFitStar, posterior_entropy, ELBO = self.ELBO_update_func(qZ,np.asscalar(goodnessOfFitStar))\n",
    "        return goodnessOfFitStar, posterior_entropy, ELBO\n",
    "        \n",
    "    def update_log_predictive_distribution(self, qZ, noiseParamStarIdx):\n",
    "        '''\n",
    "        update_log_predictive_distribution(qZ, noiseParamStarIdx)\n",
    "        inputs:\n",
    "            qZ ~ K x D\n",
    "            noiseParamStarIDx ~ int\n",
    "        \n",
    "        outputs:\n",
    "            predictiveDistribution ~ N x K+1 distribution over the K+1 possible response to each of the N windows\n",
    "            empiricalLogPredictiveDistribution ~ N x 1, this is log of predictiveDistribution sliced at empirical responses \n",
    "        '''\n",
    "        ##create object count probabilities\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K       \n",
    "        sampledZ = self.responses.Z.sample(M=self.numSamples, pZ=qZ)\n",
    "        oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "        oc_probs = self.object_count_prob_func(oc_counts, K)\n",
    "        \n",
    "        ##N x K+1, a distribution over K+1 responses to each window\n",
    "        predictiveDistribution = self.predictive_distribution_update_func(oc_probs,self.lkhdCube[noiseParamStarIdx])\n",
    "        \n",
    "        ##slice the predictive  distribution at the actual responses. if response exceeds capacity of model, prob. is 0\n",
    "        ##note that responses can be used as indices without change (unlike counts)\n",
    "        responsesAsIndices = self.curResponses.astype(intX)\n",
    "        smallEnough = map(lambda x: x <= K, responsesAsIndices)\n",
    "        empiricalPredictiveDistribution = np.where(smallEnough, predictiveDistribution[range(self.curN), responsesAsIndices.clip(0,K)],0)\n",
    "        \n",
    "#         empiricalPredictiveDistribution[smallEnough,:] = predictiveDistribution[smallEnough, responsesAsIndices[smallEnough]] ##(N x 1)\n",
    "        \n",
    "        ##N x 1\n",
    "        logEmpiricalPredictiveDistribution = np.log(empiricalPredictiveDistribution)\n",
    "        return predictiveDistribution, logEmpiricalPredictiveDistribution    \n",
    "    \n",
    "    \n",
    "    def update_percent_correct(self, predictiveDistribution):\n",
    "        \n",
    "        responses = self.curResponses.astype(intX)\n",
    "        predictions = np.argmax(predictiveDistribution, axis=1)\n",
    "        fraction_correct = np.sum(responses==predictions) / (self.curN*1.)\n",
    "        return fraction_correct*100, predictions\n",
    "    \n",
    "    def criticize(self, qZ, noiseParamStarIdx, goodnessOfFitStar, t):\n",
    "        \n",
    "        self.goodnessOfFitStar_history[t] = goodnessOfFitStar\n",
    "        _,self.posteriorEntropy_history[t], self.ELBO_history[t] = self.update_ELBO(qZ, goodnessOfFitStar)\n",
    "        \n",
    "        ##get predictive distribution\n",
    "        predictiveDistribution,logEmpiricalPredictiveDistribution = self.update_log_predictive_distribution(qZ, noiseParamStarIdx)\n",
    "        \n",
    "        ##sum to get probability of observed data under predictive distribution\n",
    "        self.lnPredictiveDistribution_history[t] = logEmpiricalPredictiveDistribution.mean()\n",
    "        \n",
    "        ##use predicitive distribution to calculate percent correct\n",
    "        self.percentCorrect_history[t],_ = self.update_percent_correct(predictiveDistribution)\n",
    "        \n",
    "        if self.percentCorrect_history[t] > self.bestPercentCorrect:\n",
    "            print '!new best!'\n",
    "            self.bestQZ = qZ.copy()\n",
    "            self.bestNoiseParam = self.noiseParamGrid[noiseParamStarIdx,:].copy()\n",
    "            self.bestPercentCorrect = self.percentCorrect_history[t]\n",
    "            self.bestlnPredictiveDistribution = self.lnPredictiveDistribution_history[t]\n",
    "        \n",
    "\n",
    "        print 'ELBO: %f' %(self.ELBO_history[t])\n",
    "        print 'goodness of fit: %f' %(goodnessOfFitStar)\n",
    "        print 'posterior_entropy: %f' %(self.posteriorEntropy_history[t])\n",
    "        print 'mean log of predictive distribution over test samples: %f' %(self.lnPredictiveDistribution_history[t])\n",
    "        print 'pecent correct over test samples: %f' %(self.percentCorrect_history[t])\n",
    "        print '\\n'\n",
    "        \n",
    "        return \n",
    "    \n",
    "        \n",
    "    ##======utilities and bookeeping\n",
    "    def rng_seed(self):\n",
    "        if not hasattr(self, 'randNumberSeed'):\n",
    "            self.randNumberSeed = np.random.randint(0,high=10**4)\n",
    "        np.random.seed(self.randNumberSeed)\n",
    "    \n",
    "    def train_test_regularize_splits(self, trainTestSplit, trainRegSplit):\n",
    "        N = self.responses.observations.shape[0]\n",
    "        shuffledIdx = np.random.permutation(N)\n",
    "        lastTestIdx = np.floor((1-trainTestSplit)*N)\n",
    "        lastRegIdx = lastTestIdx+np.floor((1-trainRegSplit)*(N-lastTestIdx))\n",
    "    \n",
    "        testIdx = shuffledIdx[:lastTestIdx]\n",
    "        regIdx = shuffledIdx[lastTestIdx:lastRegIdx]\n",
    "        trainIdx = shuffledIdx[lastRegIdx:]\n",
    "        return trainIdx, testIdx, regIdx\n",
    "    \n",
    "    def empirical_lkhd_cube(self, idx=None):\n",
    "        if idx is None:\n",
    "            respAsIdx = self.responses.observations.astype(intX)\n",
    "        else:\n",
    "            respAsIdx = self.responses.observations[idx].astype(intX)    \n",
    "        return np.squeeze(self.lkhdCube[:,respAsIdx,:])\n",
    "        \n",
    "    def update_current(self, idx):\n",
    "        self.curN = len(idx)\n",
    "        self.curIdx = idx\n",
    "        self.curIdx = self.curIdx\n",
    "        self.curResponses = self.responses.observations[self.curIdx]\n",
    "        try: ##because the computation graph is inelegant\n",
    "            self.curEmpiricalLkhdCube = self.empirical_lkhd_cube(idx=self.curIdx)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def store_learned_model(self):\n",
    "        if not hasattr(self, 'storedModels'):\n",
    "            self.storedModels = {0:copy.deepcopy(self)}\n",
    "            \n",
    "        else:\n",
    "            newModelKey = np.max(self.storedModels.keys())+1\n",
    "            self.storedModels[newModelKey] = copy.deepcopy(self)\n",
    "            del self.storedModels[newModelKey].storedModels\n",
    "        \n",
    "    ##visualize the variational posterior over pixels\n",
    "    def see_Q_Z(self, qZ, target_object_map = None, clim=[0,1]):\n",
    "        ##view: construct an image grid\n",
    "        fig = plt.figure(1, (30,10))\n",
    "        K = qZ.shape[0]\n",
    "        D1 = self.responses.Z.numPixels.D1\n",
    "        D2 = self.responses.Z.numPixels.D2\n",
    "\n",
    "        ##\n",
    "        qZAsImageStack = qZ.reshape(K,D1,D2)\n",
    "        if target_object_map is not None:\n",
    "            K += 1\n",
    "        grid = ImageGrid(fig, 111, # similar to subplot(111)\n",
    "                        nrows_ncols = (1, K), # creates grid of axes\n",
    "                        axes_pad=0.5, # pad between axes in inch.\n",
    "                        cbar_mode = 'each',\n",
    "                        cbar_pad = .05\n",
    "                        )\n",
    "        if target_object_map is not None:\n",
    "            im = grid[0].imshow(target_object_map,cmap='Dark2', interpolation = 'nearest')\n",
    "            grid[0].cax.colorbar(im)\n",
    "            for kk in range(1,K):\n",
    "                im = grid[kk].imshow(qZAsImageStack[kk-1], cmap='hot', clim=clim)\n",
    "                grid[kk].cax.colorbar(im)\n",
    "\n",
    "        else: \n",
    "            for kk in range(0,K):\n",
    "                im = grid[kk].imshow(qZAsImageStack[kk], cmap='hot', clim=clim,interpolation = 'nearest')\n",
    "                grid[kk].cax.colorbar(im)    \n",
    "    \n",
    "    ##=============RUN the actual variational inference algorithm========\n",
    "    \n",
    "    ##Note: the only arguments passed to the various methods should be those that are updated every iteration.\n",
    "    def run_VI(self, initialNoisinessOfZ, pOnInit, pOffInit, noiseParamNumber, numStarterMaps, numSamples, maxIterations, trainTestSplit, trainRegSplit, optimizeHyperParams=True, pixelNumOverMin=3, objectNumOverMin=3):\n",
    "      \n",
    "        \n",
    "        ##reset the rng seed\n",
    "        self.rng_seed()\n",
    "\n",
    "        ##divide the training, testing, regularization datasets\n",
    "        trainIdx, testIdx, regIdx = self.train_test_regularize_splits(trainTestSplit=trainTestSplit, trainRegSplit=trainRegSplit)\n",
    "\n",
    "        ##set empirical likelihoods, and responses to training set\n",
    "        self.update_current(trainIdx)\n",
    "        \n",
    "        \n",
    "        if optimizeHyperParams:\n",
    "            ##collect the arguments for use in recursive call below\n",
    "            allArgs = [initialNoisinessOfZ, pOnInit, pOffInit, noiseParamNumber, numStarterMaps, numSamples, maxIterations, trainTestSplit, trainRegSplit]\n",
    "            \n",
    "            ##initialize the ranges of hyperparameters\n",
    "            self.init_number_of_objects_range(numOverMin=objectNumOverMin)\n",
    "            self.init_pixel_resolution_range(numOverMin=pixelNumOverMin)\n",
    "            \n",
    "            ##run vi in a loop over hyperparameters\n",
    "            for k in self.numberOfObjectsRange:\n",
    "                self.responses.Z.categoryPrior.numObjects.set_value(k)\n",
    "                print '--------number of objects: %d------------' %(self.responses.Z.categoryPrior.numObjects.K)\n",
    "                for d1,d2 in self.pixelResolutionRange:\n",
    "                    self.responses.Z.numPixels.set_value(d1,d2)\n",
    "                    print '--------image resolution: (%d,%d,%d)------------' %(self.responses.Z.numPixels.D1,self.responses.Z.numPixels.D2,self.responses.Z.numPixels.D)\n",
    "                    ##recursively call run_VI, each time with a fixed set of hyperparams, avoiding the hyperparameter loop\n",
    "                    numIters=self.run_VI(*allArgs, optimizeHyperParams=False)\n",
    "                    self.numIters = numIters\n",
    "                    self.store_learned_model()\n",
    "            bestModel = self.optimize_hyper_parameters()\n",
    "            bestModel.trainIdx = trainIdx\n",
    "            bestModel.regIdx = regIdx\n",
    "            bestModel.testIdx = testIdx\n",
    "            return bestModel\n",
    "        else:\n",
    "\n",
    "\n",
    "            ##--construct the lkhdCube and slice it at the discrete value of the noise params closest to the supplied init values\n",
    "            noiseParamStarIdx = self.init_noiseParams(pOnInit, pOffInit, noiseParamNumber)\n",
    "\n",
    "            ##store numSamples for consultation in a few functions\n",
    "            self.numSamples = numSamples\n",
    "\n",
    "            ##--construct lkhd cube\n",
    "            self.lkhdCube = self.responses.make_lkhd_cube(self.noiseParamGrid[:,0],self.noiseParamGrid[:,1])  ##G x K x K\n",
    "\n",
    "            ##set current empirical likelihoods, and responses to training set (have to re-run because of lkhdCube)\n",
    "            self.update_current(trainIdx)\n",
    "\n",
    "            ##initialize variational posterior\n",
    "            qZ, self.startZ, self.starterLogs = self.init_qZ(numStarterMaps, self.curEmpiricalLkhdCube[noiseParamStarIdx], initialNoisinessOfZ)\n",
    "\n",
    "            ##initialize variational prior\n",
    "            ExpLnPi,_ = self.init_Eln_pi(qZ)\n",
    "\n",
    "            ##get initial goodnessOfFitStar\n",
    "            goodnessOfFitStar = self.update_goodness_of_fit(qZ)[noiseParamStarIdx]\n",
    "\n",
    "            ##set initial PStar, which is our term for an empirical lkhd table evaluated at the noiseParamStarIdx\n",
    "            ##PStar ~ N x K\n",
    "            PStar = self.curEmpiricalLkhdCube[noiseParamStarIdx]\n",
    "\n",
    "            ##--initialize arrays for learning histories and storing solutions\n",
    "            iteration = 0\n",
    "            self.ELBO_history = np.zeros((maxIterations+1,1))\n",
    "            self.goodnessOfFitStar_history = np.zeros((maxIterations+1,1))\n",
    "            self.posteriorEntropy_history = np.zeros((maxIterations+1,1))\n",
    "            self.percentCorrect_history = np.zeros((maxIterations+1,1))\n",
    "            self.lnPredictiveDistribution_history = np.zeros((maxIterations+1,1))\n",
    "            self.bestPercentCorrect = 0\n",
    "\n",
    "            delta_ELBO = np.inf\n",
    "            min_delta_ELBO = 10e-15\n",
    "            ELBO_old = 0.\n",
    "\n",
    "            ##--publish initial criticism. use regularization data\n",
    "            self.update_current(regIdx)\n",
    "            self.criticize(qZ, noiseParamStarIdx, goodnessOfFitStar, iteration)\n",
    "\n",
    "            ##switch back to training data\n",
    "            self.update_current(trainIdx)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "\n",
    "            while (delta_ELBO > min_delta_ELBO) and (iteration <= maxIterations):\n",
    "\n",
    "                ##put lkhd in log domain\n",
    "                lnPStar = np.log(PStar).astype(floatX)\n",
    "\n",
    "                ##coordinate ascent on variational posteriors of object map pixels\n",
    "                qZ = self.update_qZ(qZ, ExpLnPi, noiseParamStarIdx)\n",
    "\n",
    "\n",
    "                ##update noise params: calculates goodness of fit for all possible noise params, finds best,\n",
    "                ##returns it, returns best noiseParam, index of best noiseParam, and updates PStar\n",
    "                noiseParamStar, noiseParamStarIdx, PStar, goodnessOfFitStar = self.optimize_PStar(qZ)\n",
    "\n",
    "                ##update prior params\n",
    "                ExpLnPi, _ = self.update_qPi(qZ)\n",
    "\n",
    "                ##criticize using regularization data\n",
    "                self.update_current(regIdx)\n",
    "                self.criticize(qZ, noiseParamStarIdx, goodnessOfFitStar, iteration)\n",
    "\n",
    "                ##switch back to training data\n",
    "                self.update_current(trainIdx)\n",
    "\n",
    "                ##update ELBO convergence criteria\n",
    "                delta_ELBO = np.abs(self.ELBO_history[iteration]-ELBO_old)\n",
    "                ELBO_old = self.ELBO_history[iteration]\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            return iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big dumb function for importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_imagery_probe_data(*args):\n",
    "    '''\n",
    "    open_imagery_probe_data() returns a pandas dataframe with lots of info\n",
    "    \n",
    "    or\n",
    "    \n",
    "    open_imagery_probe_data(subject, state, targetImage) accesses the dataframe and gets the stuff you want\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ##which repo?\n",
    "    drive = '/mnt/fast/'\n",
    "\n",
    "    ##base directory\n",
    "    base = 'multi_poly_probes'\n",
    "\n",
    "    ##pandas dataframe with all the experimental conditions and data\n",
    "    data_place = 'data'\n",
    "    data_file = 'multi_poly_probe_data_3_subjects.pkl'\n",
    "\n",
    "    ##open experimental data: this is a pandas dataframe\n",
    "    experiment = pd.read_pickle(join(drive, base, data_place, data_file))\n",
    "    if not args:\n",
    "        return experiment\n",
    "    else:\n",
    "        subject = args[0]\n",
    "        state = args[1]\n",
    "        targetImageName = args[2]\n",
    "        ##target images\n",
    "        image_place = 'target_images'\n",
    "        mask_place = 'masks'\n",
    "\n",
    "        target_image_file = targetImageName+'_letterbox.png'\n",
    "        mask_image_file = targetImageName+'_mask.tif'\n",
    "\n",
    "        ##window files\n",
    "        window_place = 'probes'\n",
    "        window_file = targetImageName+'_letterbox_img__probe_dict.pkl'\n",
    "\n",
    "        ##open target image/object map: useful as a guide\n",
    "        targetImage = open(join(drive, base, image_place, target_image_file),mode='r').convert('L')\n",
    "\n",
    "        ##open\n",
    "        targetObjectMap = open(join(drive, base, mask_place, mask_image_file),mode='r').convert('L')\n",
    "\n",
    "\n",
    "        ##get the responses you want\n",
    "        ##responses of select subject / target image / cognitive state\n",
    "        resp = experiment[(experiment['image']==targetImageName) * (experiment['subj']==subject) * (experiment['state']==state)].response.values\n",
    "\n",
    "        ##run a nan check--who knew there would be nans in this data?\n",
    "        nanIdx = ~np.isnan(resp)\n",
    "        resp = resp[nanIdx]\n",
    "        if any(~nanIdx):\n",
    "            print 'killed some nans'\n",
    "        \n",
    "        ##give it dimensions\n",
    "        resp = resp[np.newaxis,:]\n",
    "        \n",
    "        ##corresponding window indices\n",
    "        windowIdx = experiment[(experiment['image']==targetImageName) * (experiment['subj']==subject) * (experiment['state']==state)].probe.values\n",
    "\n",
    "        ##open the windows. creates a dictionary with \"index/mask\" keys. this usually takes a while\n",
    "        windows = pd.read_pickle(join(drive, base, window_place, window_file)) ##N x D1Prime x D2Prime\n",
    "        N = len(windows['index'])\n",
    "        window_shape = windows['mask'][0].shape\n",
    "        W = np.zeros((N,window_shape[0],window_shape[1]),dtype=floatX)\n",
    "\n",
    "\n",
    "        ##correctly order and reformat\n",
    "        for ii,w in enumerate(windowIdx):\n",
    "            str_dx = map(int, w.split('_'))\n",
    "            dx = windows['index'].index(str_dx)\n",
    "            W[ii] = windows['mask'][dx].clip(0,1)\n",
    "            \n",
    "        ##run another nan check\n",
    "        W = W[nanIdx]\n",
    "\n",
    "        return W, resp, experiment, targetObjectMap,targetImage\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
