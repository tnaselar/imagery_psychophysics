{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we develop some new metrics and test them on fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagery_psychophysics.src.variational as very\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "from PIL.Image import open as open_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up basic model parameters and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model variable parameters\n",
    "K = 3\n",
    "D1,D2 = 8,8\n",
    "dispersion = 2.2\n",
    "pon,poff = 0.95, .05\n",
    "\n",
    "##number of objects\n",
    "nObj = very.numObjects()\n",
    "nObj.set_value(K)\n",
    "\n",
    "##dispersion on category prior\n",
    "pDisp = very.priorDispersion()\n",
    "pDisp.set_value(dispersion)\n",
    "\n",
    "##resolution of object map Z\n",
    "nPixels = very.numPixels()\n",
    "nPixels.set_value(D1,D2)\n",
    "\n",
    "##category prior and object map\n",
    "catProb = very.categoryProbs(nObj,pDisp)\n",
    "catProb.set_value(catProb.sample())\n",
    "Z = very.latentObjMap(catProb,nPixels)\n",
    "originalZ = copy.deepcopy(Z)\n",
    "\n",
    "##create a sample of an object map\n",
    "targetZ = Z.sample(M=1)\n",
    "\n",
    "##noise params: we don't set these because we're going to learn them\n",
    "nP = very.noiseParams()\n",
    "\n",
    "##windows\n",
    "DPrime1,DPrime2 = 8*D1, 8*D2 ##this gives the \"native resolution\" of the windows\n",
    "shape = (DPrime1,DPrime2)\n",
    "baseShape = (8, 8) ##size in pixels of the smallest probes\n",
    "numScales = 4 ##number of probe sizes between smallest and native resolution\n",
    "stride = 1 ##how far each probe travels when constructing probes, as a fraction of probe size\n",
    "numRandProbes = 170 ##number of non-contiguous probes\n",
    "randProbeOrder = (2, 4) ##non-contig probes will contain this many patches (range of)\n",
    "windows = very.probes() ##instantiate a windows object\n",
    "W = windows.make_windows(shape, baseShape, numScales, stride, numRandProbes, randProbeOrder) ##create the windows\n",
    "\n",
    "##now, we want to downsample the windows to a more manageable \"working\" resolution.\n",
    "##to do this, we first calculate all of the downsamples that have integer dimensions and preserve the aspect ratio\n",
    "##we set workingScale=n to choose the nth smallest resolution as our working resolution. \n",
    "##NOTE: THIS DOESN'T REALLY WORK BECAUSE EACH NATIVE RESOLUTIONI NEEDS TO BE CLEANLY DIVISIBLE BY WORKING RESOLUTION.\n",
    "##FOR THE \"UPSAMPLING\" OF Z TO WORK. SO, BEST JUST TO WORK WITH NATIVE RESOLUTIONS THAT ARE POWERS OF SOME NUMBER.\n",
    "##LIKE, SAY, 2.\n",
    "#resolutions, workingResolution = windows.resolve(shape, workingScale=-1) \n",
    "\n",
    "##Given the above, we'll just set the working resolution by hand\n",
    "workingResolution = (64,64)\n",
    "\n",
    "##next, we downsample the windows, and set_value\n",
    "windows.set_value(windows.reshape(W, workingResolution),flatten=True)\n",
    "\n",
    "##response object\n",
    "r = very.responses(Z,nP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'native resolution: (%d,%d)' %(D1,D2)\n",
    "print 'target resolution: (%d,%d)' %shape\n",
    "print 'working resolution: (%d,%d)' %workingResolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in an image from a real experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##a real target from a recent experiment\n",
    "##which repo?\n",
    "drive = '/home/tnaselar/FAST/imagery_psychophysics'\n",
    "\n",
    "##base directory\n",
    "base = 'multi_poly_probes'\n",
    "\n",
    "##pandas dataframe with all the experimental conditions and data\n",
    "data_place = 'data'\n",
    "data_file = 'multi_poly_probe_data_3_subjects.pkl'\n",
    "subject = 'KL'\n",
    "state = 'pcp'\n",
    "\n",
    "\n",
    "##target images\n",
    "image_place = 'masks'\n",
    "target_image_name = 'candle_01'\n",
    "target_image_file = 'candle_mask_mostbjs.tif'\n",
    "\n",
    "##window files\n",
    "window_place = 'probes'\n",
    "window_file = 'candle_01_letterbox_img__probe_dict.pkl'\n",
    "\n",
    "\n",
    "##open\n",
    "test_object_map = open_image(join(drive, base, image_place, target_image_file),mode='r').convert('L')\n",
    "\n",
    "##record K\n",
    "values = np.array(np.unique(test_object_map))\n",
    "targetK = len(values)\n",
    "\n",
    "##resize to window, checking for preserved K\n",
    "test_object_map=test_object_map.resize((test_object_map.size[1], test_object_map.size[0]),)\n",
    "values = np.array(np.unique(test_object_map))\n",
    "assert targetK==len(values)\n",
    "\n",
    "##digitize, checking \n",
    "test_object_map=np.digitize(test_object_map, bins=values, right=True ).astype(int)\n",
    "values = np.array(np.unique(test_object_map))\n",
    "assert targetK==len(values)\n",
    "\n",
    "##one-hot encoding\n",
    "fullscreenSize = np.prod(test_object_map.size)\n",
    "test_object_map = np.eye(K)[test_object_map.ravel()].T.reshape((1,targetK,fullscreenSize))\n",
    "\n",
    "##view\n",
    "test_object_map_image = see_Z_sample(test_object_map[0], window_shape, show=False)\n",
    "plt.imshow(test_object_map_image, cmap='Dark2')\n",
    "plt.imshow(W[-5].reshape(window_shape).astype('uint8')*255, interpolation='none', alpha = .2, cmap=plt.cm.gray, clim=[0,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_object_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fake responses\n",
    "r.set_values(windows=windows)\n",
    "data = r.sample(targetZ,pon,poff)\n",
    "r.set_values(data=data)\n",
    "\n",
    "print 'total observations: %d' %(r.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r.observations,bins=range(0,K+2),rwidth = .5, align='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the fake responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##instantiate the variational inferences we want to perform\n",
    "iqZ = very.inferQZ()\n",
    "iqPi = very.inferQPi()\n",
    "\n",
    "##...and the parameter optimizations (point-estimate) we want\n",
    "oNP = very.optimizeNoiseParams()\n",
    "\n",
    "##variational inference combines them all together\n",
    "vi = very.VI(r, iqZ,oNP, iqPi)\n",
    "\n",
    "### Run variational inference\n",
    "\n",
    "##inference algorithm parameters\n",
    "initialNoisinessOfZ = 0.2\n",
    "pOn_init, pOff_init = .95, 0.05\n",
    "densityOfNoiseParamGrid = 50\n",
    "numStarterMaps = 20\n",
    "numSamplesForComputingObjectCountProbs = 4\n",
    "maxNumIterations = 100\n",
    "trainTestSplit = 1.0\n",
    "trainRegSplit = .8\n",
    "pixelNumOverMin = 2\n",
    "objectNumOverMin = 2\n",
    "\n",
    "bestModel = vi.run_VI(initialNoisinessOfZ, \\\n",
    "                     pOn_init, pOff_init, \\\n",
    "                     densityOfNoiseParamGrid, \\\n",
    "                     numStarterMaps, \\\n",
    "                     numSamplesForComputingObjectCountProbs, \\\n",
    "                     maxNumIterations, \\\n",
    "                     trainTestSplit, trainRegSplit, \\\n",
    "                     optimizeHyperParams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.bestPercentCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.bestNoiseParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=Z.view_sample(targetZ,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.see_Q_Z(vi.bestQZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View target and posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel entropy maps\n",
    "Just map the entropy of the posterior at each pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute KL divergence (crossentropy, entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute pixelwise co-membership probability maps (pixelcom map)\n",
    "For each pixel, map the probability of all other pixels belonging to same object.\n",
    "Create a stack of such \"pixelcom\" maps.\n",
    "Real images will have degenerate (i.e., binary) pixelcom maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the pixelcom maps to reconstruct target object maps\n",
    "Simply average them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the pixelcom maps to perform image identification\n",
    "Some how compare the pixelcom maps for real images to the pixelcom maps derived from vision/imagery experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the pixelcom maps to estimate size differences and translational differences between objects in vision and imagery\n",
    "take the \"real\" object from the target image map. \n",
    "estimate probabilities under enlargements and translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------- Tossed ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##==========construct model random variables\n",
    "# ##number of objects\n",
    "# nObj = numObjects()\n",
    "\n",
    "# ##dispersion on category prior: here we set a hyperprior\n",
    "# pDisp = priorDispersion()\n",
    "# dispersion = 1.0\n",
    "# pDisp.set_value(dispersion)\n",
    "\n",
    "# ##resolution of object map Z\n",
    "# nPixels = numPixels()\n",
    "\n",
    "# ##category prior and object map\n",
    "# catProb = categoryProbs(nObj,pDisp)\n",
    "# Z = latentObjMap(catProb,nPixels)\n",
    "\n",
    "# ##noise params\n",
    "# nP = noiseParams()\n",
    "\n",
    "# ##windows: we change their shape a little to make them easier to work with\n",
    "# desiredWindowShape = (375,600)\n",
    "# workingScale = 5\n",
    "# w = probes()\n",
    "# resolutions, workingResolution = w.resolve(desiredWindowShape, workingScale)\n",
    "# w.set_value(w.reshape(windows, workingResolution),flatten=True)\n",
    "# print 'working resolution is (%d, %d)' %(workingResolution[0], workingResolution[1])\n",
    "\n",
    "\n",
    "# ##response object\n",
    "# r = responses(Z,nP)\n",
    "\n",
    "# ##fake data\n",
    "# r.set_values(windows=w)\n",
    "\n",
    "# r.set_values(data=resp)\n",
    "\n",
    "# print 'total observations: %d' %(r.N)\n",
    "\n",
    "# tMap = target_image()\n",
    "# targetObjectMap_test, targetImage_test = tMap.reshape(targetObjectMap,workingResolution,targetImage=targetImage)\n",
    "# tMap.set_values(targetObjectMap_test, targetImage_test)\n",
    "# ##=============\n",
    "\n",
    "# ##instantiate the variational inferences we want to perform\n",
    "# iqZ = inferQZ()\n",
    "# iqPi = inferQPi()\n",
    "\n",
    "# ##...and the parameter optimizations (point-estimate) we want\n",
    "# oNP = optimizeNoiseParams()\n",
    "\n",
    "# ##variational inference combines them all together\n",
    "# vi = VI(r, iqZ,oNP, iqPi)\n",
    "\n",
    "# ### Run variational inference\n",
    "\n",
    "# ##inference algorithm parameters\n",
    "# initialNoisinessOfZ = 0.2\n",
    "# pOn_init, pOff_init = .8, 0.2\n",
    "# densityOfNoiseParamGrid = 50\n",
    "# numStarterMaps = 20\n",
    "# numSamplesForComputingObjectCountProbs = 4\n",
    "# maxNumIterations = 50\n",
    "# trainTestSplit = 1.0\n",
    "# trainRegSplit = .8\n",
    "# pixelNumOverMin = 2\n",
    "# objectNumOverMin = 2\n",
    "\n",
    "# print '=========================(subject, state, target) = (%s, %s, %s) ====' %(subject, state, targetImageName)\n",
    "# bestModel = vi.run_VI(initialNoisinessOfZ, \\\n",
    "#                      pOn_init, pOff_init, \\\n",
    "#                      densityOfNoiseParamGrid, \\\n",
    "#                      numStarterMaps, \\\n",
    "#                      numSamplesForComputingObjectCountProbs, \\\n",
    "#                      maxNumIterations, \\\n",
    "#                      trainTestSplit, trainRegSplit, \\\n",
    "#                      optimizeHyperParams=True, pixelNumOverMin=pixelNumOverMin, objectNumOverMin=objectNumOverMin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
