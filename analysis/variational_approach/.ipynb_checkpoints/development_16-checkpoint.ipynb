{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new set of classes for mental image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagery_psychophysics.utils.variational_utils import *\n",
    "from imagery_psychophysics.src.variational import responses, noiseParams, probes, inferQZ, optimizeNoiseParams\n",
    "from imagery_psychophysics.src.variational import to_one_hot_func\n",
    "from object_parsing.src.image_objects import object_box, mask_quantize\n",
    "from skimage.transform import AffineTransform, matrix_transform, resize, SimilarityTransform\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.stats import poisson\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from PIL.Image import fromarray\n",
    "from scipy.linalg import expm\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from theano import tensor as T\n",
    "from theano.tensor.extra_ops import repeat, to_one_hot\n",
    "from theano import function, shared\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "floatX = 'float32'\n",
    "intX = 'int32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows, resp, _, targetObjectMap,targetImage = open_imagery_probe_data('KL', 'pcp', 'peaches_05')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_points(shape):\n",
    "    D1,D2 = shape\n",
    "    sourceI, sourceJ= np.meshgrid(range(D1),range(D2))\n",
    "    sourcePoints = np.array([sourceJ.flatten(),sourceI.flatten()]).T.astype('float64')\n",
    "    return sourcePoints\n",
    "\n",
    "\n",
    "#box = (left, upper, right, lower)\n",
    "def get_box_center(boxtuple):\n",
    "    left, upper, right, lower = boxtuple\n",
    "    center_row = np.median([upper, lower])\n",
    "    center_col = np.median([left, right])\n",
    "    return np.array([center_row, center_col])\n",
    "\n",
    "def compose_and_iterate(sourcePoints, transformFuncList, activations=None, iterations=1):\n",
    "    displacedPoints = copy(sourcePoints)\n",
    "    if iterations:\n",
    "        ##apply transforms in series, noting activations, and assuming that xForm manages control point arithmetic\n",
    "        if activations is None:\n",
    "            activations = [1]*len(transformFuncList)\n",
    "        for xForm,a in zip(transformFuncList,activations):\n",
    "            if not a:\n",
    "                xForm = lambda x: x\n",
    "            displacedPoints = xForm(displacedPoints)\n",
    "        ##reduce iterations\n",
    "        iterations -= 1\n",
    "        return compose_and_iterate(displacedPoints, transformFuncList, activations, iterations=iterations)\n",
    "    else:\n",
    "        return displacedPoints\n",
    "\n",
    "        \n",
    "\n",
    "def local_affine_warp(sourcePoints, controlPoint, baseTransformMatrix, scaleFunction):\n",
    "    \n",
    "    r = pairwise_distances(sourcePoints, controlPoint)\n",
    "    scalars = scaleFunction(r)  \n",
    "    scaledXForms = scalars[:,:,np.newaxis]*baseTransformMatrix\n",
    "    xForms = np.array(map(expm, scaledXForms))\n",
    "\n",
    "    displacedPoints = np.sum((sourcePoints[:,:,np.newaxis]-controlPoint.T)*xForms,axis=2)+controlPoint\n",
    "\n",
    "    return displacedPoints,xForms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for globally affine distortions\n",
    "def global_affine_distortion_set(big=1.2, small=.8, rotate=np.pi/16., shear = np.pi/16., translate=3):\n",
    "    big = 1.2\n",
    "    small = .8\n",
    "    rotate = np.pi/16.\n",
    "    shear = np.pi/16.\n",
    "    translate = 3.\n",
    "    globalAffineParamDictList = []\n",
    "    globalAffineParamDictList += [{'scale':(big,1.), 'rotation':None, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':(1.,big), 'rotation':None, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':(small,1.), 'rotation':None, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':(1.,small), 'rotation':None, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':rotate, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':-rotate, 'shear':None, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':rotate, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':-rotate, 'translation':None}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(translate,0)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(0, translate)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(-translate, 0)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(0, -translate)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(translate, -translate)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(-translate, translate)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(translate, translate)}]\n",
    "    globalAffineParamDictList += [{'scale':None, 'rotation':None, 'shear':None, 'translation':(-translate, translate)}]\n",
    "    return globalAffineParamDictList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activationPrior(object):\n",
    "    def __init__(self):\n",
    "        self._activationProb = T.scalar('activationProb',dtype=floatX)\n",
    "        \n",
    "    def set_value(self,value):\n",
    "        assert value <= 1\n",
    "        assert value >= 0\n",
    "        self.activationProb = np.array(value,dtype=floatX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iterationPrior(object):\n",
    "    def __init__(self):\n",
    "        self._iterationMean = T.scalar('iterationMean', dtype=floatX)        \n",
    "        \n",
    "    def set_value(self,value):\n",
    "        assert value >= 0\n",
    "        self.iterationMean = np.array(value,dtype=floatX)\n",
    "    \n",
    "    def pmf(self, num):\n",
    "        return poisson.pmf(num,self.iterationMean)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseObjectMap(object):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,objectMap):\n",
    "        from PIL.Image import fromarray\n",
    "        '''\n",
    "        baseObjectMap(objectMap)\n",
    "        an object map in cannonical form, i.e., a D1 x D2 discrete numpy array\n",
    "        \n",
    "        '''\n",
    "        self.objectMap = self._convert_object_labels(objectMap)\n",
    "        self.D1 = self.objectMap.shape[0]\n",
    "        self.D2 = self.objectMap.shape[1]\n",
    "        self.D = np.prod([self.D1,self.D2])\n",
    "        self.sourcePoints = source_points((self.D1,self.D2))\n",
    "        self.objectCenters = self._get_object_centers()\n",
    "        \n",
    "        ##theano expressions\n",
    "        self._D1 = T.scalar('numPixelRows',dtype=intX) ##num pixel rows\n",
    "        self._D2 = T.scalar('numPixelCols',dtype=intX) ##num pixel cols\n",
    "        self._D = self._D1*self._D2 ##total number of pixels\n",
    "        self._D.name = 'numPixels'\n",
    "        self._K = T.scalar('numObjects',dtype=intX) ##a theano integer scalar       \n",
    "    \n",
    "    def _convert_object_labels(self,objectMap):\n",
    "        self.nativeLabelList = np.unique(objectMap).tolist()\n",
    "        self.K = len(self.nativeLabelList)\n",
    "        return np.digitize(objectMap, self.nativeLabelList,right=True)\n",
    "        \n",
    "        \n",
    "    def _get_object_centers(self):        \n",
    "        objectCenters = np.empty((self.K,2),dtype=floatX)\n",
    "        for objectLabel in range(self.K):\n",
    "            box = object_box(fromarray(self.objectMap.astype('uint8')),objectLabel)\n",
    "            objectCenters[objectLabel,:] = get_box_center(box)[np.newaxis,:].astype(floatX)\n",
    "        return objectCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class distortions(object):\n",
    "    def __init__(self, controlPoints,kind='global',distortionParamDictList=[{'scale':None, 'rotation':None, 'shear':None, 'translation':None}],scale=10.):\n",
    "\n",
    "        '''\n",
    "        distortions(controlPoints,\n",
    "                    kind='global',\n",
    "                    distortionParamDictList=[{'scale':None,\n",
    "                                              'rotation':None,\n",
    "                                              'shear':None, \n",
    "                                              'translation':None}],\n",
    "                                              scale=10.))\n",
    "        the basic job of this class is to create a list of image distortions.\n",
    "        these distortions will be composed and applied to an image downstream.\n",
    "        \n",
    "        inputs:\n",
    "        \n",
    "        controlPoints ~ M x 2 array of points about which the distortions will be applied\n",
    "        \n",
    "        distortionParamDistList ~ list of lenght G of dicts with distortion params.\n",
    "            {'scale':None, 'rotation':None, 'shear':None, 'translation':None}\n",
    "            default is the id matrix. scale takes (sx,sy), values <0 shrink.\n",
    "            translation takes (tx,ty).\n",
    "            shear and rotation take angles in radians.\n",
    "\n",
    "        kind ~ \"local\" or \"global\". if local, use a locally affine warping approach. if\n",
    "            global, interpret params as globally affine. Note: \"global\" is MUCH faster.\n",
    "        \n",
    "        scale ~ scalar. if kind = 'local', this defines locality. otherwise ignore.\n",
    "            Note: if small, very local and distortions are locally \"strong\". \n",
    "            if big, distortions apply almost globally but are very \"weak\".\n",
    "            \n",
    "        attributes:\n",
    "        \n",
    "        transformFuncList ~ list of warping functions like warpfunc(sourcePoints)\n",
    "            where sourcePoints is D x 2 array of points to displace. these functions\n",
    "            should return an D x 2 array of \"displaced points\" that will be used\n",
    "            downstream to interpolate. the lenght of this list will be MxG.\n",
    "            this is the number of controlPoints (M) times number of distortions (G).\n",
    "            all distortions for one control point are in contiguous segment of the list.\n",
    "      \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        self.kind = kind\n",
    "        self.controlPoints = controlPoints\n",
    "        self.numControlPoints = controlPoints.shape[0]\n",
    "        self.distortionParamDictList = distortionParamDictList\n",
    "        self.numDistortions = len(self.distortionParamDictList)\n",
    "        self._get_transform_function_list()\n",
    "        \n",
    "    def _get_transform_function_list(self):\n",
    "        self.transformFuncList = []\n",
    "        if self.kind == 'global':\n",
    "            for ctrlPt in self.controlPoints:\n",
    "                for paramDict in self.distortionParamDictList:\n",
    "                    xForm = AffineTransform(**paramDict)\n",
    "                    self.transformFuncList += [lambda x,bc=ctrlPt,xf=xForm.params.astype(floatX): matrix_transform(x-bc,xf)+bc]\n",
    "        elif self.kind == 'local':\n",
    "            scaleFunc = lambda r: norm.pdf(r,scale=scale)\n",
    "            for ctrlPt in self.controlPoints:\n",
    "                for paramDict in self.distortionParamDictList:\n",
    "                    xForm = AffineTransform(**paramDict)\n",
    "                    baseTransformMatrix = xForm.params[:2,:2].astype(floatX)\n",
    "                    self.transformFuncList += [lambda x, btm=baseTransformMatrix, cp=ctrlPt: local_affine_warp(x,cp,btm, scaleFunc)[0]]\n",
    "        else:\n",
    "            raise ValueError('keyword \"kind\" must be either \"global\" or \"local\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activations(object):\n",
    "    def __init__(self, activationPrior_inst, distortions_inst):\n",
    "        self.activationPrior = activationPrior_inst\n",
    "        self.distortions = distortions_inst\n",
    "        self._A = T.vector('activationVector', dtype=intX)\n",
    "        self.numActivationVars = self.distortions.numControlPoints*self.distortions.numDistortions\n",
    "        self.numActivationStates = 2  #can be on or off (1, 0)\n",
    "    \n",
    "    def sample(self, M = 1, pval=None, kind='1hot'):\n",
    "        '''\n",
    "        sample(M = 1, pval=None)\n",
    "        \n",
    "        if pval supplied, should be numActivationStates x numActivationVars array\n",
    "        each colum is probability that activation = [1,0]. top row is ON, bottom is OFF,\n",
    "        bottom = 1-top.\n",
    "        if not supplied, use prior probability to sample.\n",
    "        \n",
    "        if kind = '1hot', returns an M x numActivationStates x numActivationVars array\n",
    "        if kind = 'canonical', returns an M x numberActivationVars array, each element a 0 or 1\n",
    "        '''\n",
    "        if pval is None:\n",
    "            pval = self.activationPrior.activationProb\n",
    "            pval = np.array([pval, 1-pval],ndmin=1)\n",
    "            activations = np.random.multinomial(1, pval, size=(M,self.numActivationVars)).transpose([0,2,1])\n",
    "        else:\n",
    "            assert pval.shape == (self.numActivationStates, self.numActivationVars)\n",
    "            activations = np.empty((M,self.numActivationStates, self.numActivationVars), dtype=intX)\n",
    "            for v in range(self.numActivationVars):\n",
    "                activations[:, :, v] = np.random.multinomial(1,pval[:,v], size=M)\n",
    "        if kind == '1hot':\n",
    "            return activations\n",
    "        elif kind == 'canonical':\n",
    "            return np.argmin(activations,axis=1) ##argmin becuase ON is coded as first element of each column.\n",
    "        else:\n",
    "            raise ValueError('keyword \"kind\" must be \"canonical\" or \"1hot\"')\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this is really a class for the variational posterior. it samples from\n",
    "##a discrete distribution over \"number of iterations\". whereas the prior over number of iterations\n",
    "##is a poisson, and the true posterior over number of iterations is wtfk.\n",
    "class iterations(object):\n",
    "    def __init__(self, iterationPrior_inst, numIterationVars=1,numIterationStates=3):\n",
    "        self.iterationPrior = iterationPrior_inst\n",
    "        self._t = T.vector('iterationVector',dtype=intX)\n",
    "        #for now we only support one \"global\" iteration variable.\n",
    "        #can range from 0 (no distortion) to upper limit numIterationStates\n",
    "        self.numIterationVars = numIterationVars\n",
    "        self.numIterationStates = numIterationStates\n",
    "    \n",
    "    def sample(self, pval=None, M=1, kind='1hot'):\n",
    "        '''\n",
    "        sample(pval, M=1)\n",
    "        \n",
    "        inputs:\n",
    "        pval ~ array of probabilities, shape = (numIterationStates,numIterationVars)\n",
    "            each column is a 1hot encoding of probability of number of iterations from 0 up to some\n",
    "            truncated value (numberIterationStates)\n",
    "            if pval=None, sample from uniform distribution\n",
    "            \n",
    "        M ~ number of samples\n",
    "        kind ~ \"cannoical\" or \"1hot\"\n",
    "\n",
    "        \n",
    "        returns:\n",
    "        iterations ~ if kind=\"canonical\", an M x numIterationVars array, each element is number of iterations.\n",
    "            if kind = \"1hot\", an M x numIterationStates x numIterationVars array. each for a fixed sample, \n",
    "            the column is a 1-hot encoding of number of iterations. so like iterations[m,k,n] = 1 would mean\n",
    "            that for the m_th sample, the number of iterations for the n_th iteration variable is k.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        if pval is None:\n",
    "            pval = np.empty((self.numIterationStates, self.numIterationVars), dtype=floatX)\n",
    "            pval[:] = 1./self.numIterationStates\n",
    "        assert pval.shape == (self.numIterationStates, self.numIterationVars)\n",
    "        iterations = np.empty((M, self.numIterationStates, self.numIterationVars), dtype=intX)\n",
    "        for v in range(self.numIterationVars):\n",
    "            iterations[:, :, v] = np.random.multinomial(1,pval[:,v], size=M)\n",
    "        if kind == '1hot':\n",
    "            return iterations\n",
    "        elif kind == 'canonical':\n",
    "            return np.argmax(iterations,axis=1)\n",
    "        else:\n",
    "            raise ValueError('keyword \"kind\" must be \"canonical\" or \"1hot\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class latentObjectMap(object):\n",
    "    def __init__(self, activations_inst, iterations_inst, baseObjectMap_inst):\n",
    "        self.activations = activations_inst\n",
    "        self.iterations = iterations_inst\n",
    "        self.baseObjectMap = baseObjectMap_inst\n",
    "        self._Z = T.tensor3('Z',dtype=intX) #M x K x D stack of object maps\n",
    "        self._K = T.scalar('numObjects',dtype='int32') ##a theano integer scalar\n",
    "        self._D1 = T.scalar('numPixelRows',dtype='int32') ##num pixel rows\n",
    "        self._D2 = T.scalar('numPixelCols',dtype='int32') ##num pixel cols\n",
    "        self._D = self._D1*self._D2 ##total number of pixels\n",
    "        self._D.name = 'numPixels'\n",
    "         \n",
    "    def sample(self, M=1, activationProbs=None, iterationProbs=None, kind = '1hot'):\n",
    "        '''\n",
    "        sample(M=1, activationProbs=None, iterationProbs=None, kind = '1hot')\n",
    "        generate M samples of a probabilistically distorted object map\n",
    "        \n",
    "        inputs:\n",
    "        M ~ int, number of samples\n",
    "        activationProbs ~ nActStates x nActVars array of probabilities, columns sum to 1\n",
    "        iterationProbs ~ nItStates x nItVars array of probabilities, columns sum to 1\n",
    "        kind = '1hot', 'canonical'\n",
    "        \n",
    "        returns:\n",
    "        warped,activations,iterations\n",
    "        \n",
    "        if kind == 'canonical', warped is M x D1 x D2 array of object maps.\n",
    "        if kind == '1hot', warped is M x K x D array of 1hot-encoded object maps, K=number of objects, D=D1*D2\n",
    "        \n",
    "        activations is M x nActVars\n",
    "        iterations is M x nItVars\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        D1,D2 = self._get_dims()\n",
    "        D = D1*D2\n",
    "        K = self._get_num_objects()\n",
    "        baseMap = self._get_base_map().flatten()\n",
    "        activations = self.activations.sample(M=M,pval=activationProbs,kind='canonical')\n",
    "        iterations = self.iterations.sample(M=M, pval=iterationProbs,kind='canonical')\n",
    "        sourcePoints = self.baseObjectMap.sourcePoints\n",
    "        xFormFuncList = self.activations.distortions.transformFuncList\n",
    "        objectMapStackCanonical = np.empty((M,D1,D2),dtype=intX)\n",
    "        for m in range(M):\n",
    "            actList = activations[m].tolist()\n",
    "            iters = np.copy(iterations[m])\n",
    "            displacedPoints = compose_and_iterate(sourcePoints, xFormFuncList, actList, iters)\n",
    "            objectMapStackCanonical[m,:,:] = griddata(displacedPoints, baseMap, sourcePoints, method='nearest').reshape((D1,D2))\n",
    "        if kind == '1hot':\n",
    "            objectMapStackOneHot = np.empty((M,K,D),dtype=intX)\n",
    "            for m in range(M):\n",
    "                objectMapStackOneHot[m,:,:] = to_one_hot_func(objectMapStackCanonical[m].flatten(),K).T\n",
    "            return objectMapStackOneHot, activations, iterations\n",
    "        elif kind == 'canonical':\n",
    "            \n",
    "            return objectMapStackCanonical, activations,iterations\n",
    "        else:\n",
    "            raise ValueError('keyword argument \"kind\" must be \"1hot\", or \"canonical\"')\n",
    "    \n",
    "    def convert1hot_to_image(self, sample1hot, show=False):\n",
    "        '''\n",
    "        convert1hot_to_image(sample1hot, show=False)\n",
    "        takes a 1 x K x D sample of a 1hot encoded object map \n",
    "        and converts it to a D1 x D2 image of the map.\n",
    "\n",
    "        makes plot if show=True\n",
    "        '''\n",
    "        D1,D2 = self._get_dims()\n",
    "        sampleImage = np.argmax(sample1hot,axis=1).reshape((D1,D2))\n",
    "        if show:\n",
    "            plt.imshow(sampleImage, interpolation='nearest')\n",
    "        return sampleImage   \n",
    "    \n",
    "    def _get_dims(self):\n",
    "        return self.baseObjectMap.objectMap.shape\n",
    "    def _get_num_objects(self):\n",
    "        return self.baseObjectMap.K\n",
    "    def _get_base_map(self):\n",
    "        return self.baseObjectMap.objectMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class behavior(responses):\n",
    "    def __init__(self,latentObjMap_inst, noiseParams_inst):\n",
    "        ##okay so this is super gross, but these few lines of code allow me to not have to\n",
    "        ##rewrite the whole class just to change a few variable names\n",
    "        from imagery_psychophysics.src.variational import numObjects, numPixels, priorDispersion, categoryProbs\n",
    "        cp = categoryProbs(numObjects(),priorDispersion())\n",
    "        latentObjMap_inst.categoryPrior = cp\n",
    "        latentObjMap_inst.numPixels = numPixels()\n",
    "        super(behavior, self).__init__(latentObjMap_inst, noiseParams_inst)\n",
    "        \n",
    "        ##overwrites because the graph is now different. this allows us to reuse a lot of code\n",
    "        self.Z.categoryPrior.numObjects._K = latentObjMap_inst._K\n",
    "        self.Z.numPixels._D1 = latentObjMap_inst._D1\n",
    "        self.Z.numPixels._D2 = latentObjMap_inst._D2\n",
    "        self.Z.numPixels._D  = latentObjMap_inst._D\n",
    "        self.Z.categoryPrior.numObjects.K = latentObjMap_inst._get_num_objects()\n",
    "        self.Z.numPixels.D1,self.Z.numPixels.D2 = latentObjMap_inst._get_dims()\n",
    "        self.Z.numPixels.D = np.prod(latentObjMap_inst._get_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "class variationalImageRegistration(object):\n",
    "    def __init__(self, behavior_inst, activationInferQZ_inst, iterationInferQZ_inst, optimizeNoiseParams_inst):\n",
    "        self.responses = behavior_inst\n",
    "        self.iQA = activationInferQZ_inst\n",
    "        self.iQt = iterationInferQZ_inst\n",
    "        self.oNP = optimizeNoiseParams_inst\n",
    "\n",
    "        ##hard-coded bounds for clipping floating point errors\n",
    "        self._a_min = 10e-10\n",
    "        self._a_max = 1\n",
    "        \n",
    "        \n",
    "        ##compile theano expressions, functions\n",
    "        self.compile_object_probs()\n",
    "        self.compile_entropy()\n",
    "        self.compile_predictive_distribution()\n",
    "        self.compile_goodness_of_fit()\n",
    "        self.compile_ELBO()\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    ##===compile theano expressions, functions===\n",
    "    def compile_object_probs(self):\n",
    "        _object_counts = self.responses._object_counts ##(M x N)\n",
    "        _K = self.responses.Z.categoryPrior.numObjects._K\n",
    "        \n",
    "        ##non-normalized object count probs (N x K)\n",
    "        ##we are using these counts as indices, so we have to subtract 1\n",
    "        _object_count_prob_nn = to_one_hot(_object_counts.astype('int32').flatten()-1,_K).reshape((_object_counts.shape[0],_object_counts.shape[1],_K)).sum(axis=0)\n",
    "\n",
    "        ##object count probs (N x K)\n",
    "        _object_count_prob = _object_count_prob_nn / _object_count_prob_nn.sum(axis=1).reshape((_object_count_prob_nn.shape[0], 1))\n",
    "        \n",
    "        self.object_count_prob_func = function([_object_counts, _K], outputs = _object_count_prob)\n",
    "        \n",
    "    def compile_goodness_of_fit(self):\n",
    "        _P_theta = T.tensor3('_P_theta') ##(G x N x K)\n",
    "        _oc_probs = T.matrix('oc_probs') ##N x K ~ this is a place holder for the \"object count prob\" matrix\n",
    "\n",
    "\n",
    "        ##(G x N x K)\n",
    "        ##(    N x K)  (dot product, broadcast across G)\n",
    "        ##(G x 1)  --> because we don't do vectors we reshape to make output 2Dimensional (G x 1)\n",
    "\n",
    "        ##log of likelihood\n",
    "\n",
    "        _ln_P_theta = T.log(_P_theta)\n",
    "\n",
    "        ##ln [q(theta+, theta-) - const (G x 1) ], the log of the unormalized variational distribution over theta\n",
    "        ##currently we're taking a point estimate on theta so we don't do the hard normalization step.\n",
    "        _goodnessOfFit = T.tensordot(_ln_P_theta, _oc_probs, axes=[[1,2], [0,1]],).reshape((_P_theta.shape[0], 1))\n",
    "        \n",
    "        ##returns G x 1 goodness of fit array. each element gives goodness of fit for one possible value of noiseparams\n",
    "        self.goodness_of_fit_func = function([_P_theta, _oc_probs], outputs = _goodnessOfFit)\n",
    "        \n",
    "    def compile_entropy(self):\n",
    "        ##qA and then qt\n",
    "        ##the order should always be numStates x numVars to be consistent with K x D format of previous code.\n",
    "        _qA = T.matrix('qA_holder') ##numActivationStates x numActivationVars\n",
    "        _qt = T.matrix('qt_holder') #numIterationStates x numIterationVars\n",
    "        a_min = self._a_min\n",
    "        a_max = self._a_max\n",
    "        ##scalar: entropy of joint variational posterior over A, t\n",
    "        _activation_entropy = -T.tensordot(_qA.clip(a_min,a_max), T.log(_qA.clip(a_min,a_max)))\n",
    "        _iteration_entropy = -T.tensordot(_qt.clip(a_min,a_max), T.log(_qt.clip(a_min,a_max)))\n",
    "        _posterior_entropy = _activation_entropy+_iteration_entropy\n",
    "        self.entropy_update_func = function([_qA,_qt], outputs=_posterior_entropy)\n",
    "        \n",
    "     \n",
    "    def compile_ELBO(self):\n",
    "        _goodnessOfFitStar = T.scalar('_goodnessOfFitStar') ##scalar\n",
    "        \n",
    "        _posterior_entropy = T.scalar('entropy_holder') \n",
    "\n",
    "        ##scalar\n",
    "        _ELBO = _goodnessOfFitStar  + _posterior_entropy\n",
    "        self.ELBO_update_func = function([_goodnessOfFitStar, _posterior_entropy], outputs=[_goodnessOfFitStar, _posterior_entropy, _ELBO])\n",
    "\n",
    "    def compile_predictive_distribution(self):\n",
    "        _lkhdTable = T.matrix('lkhd_table_pred') ##K+1 x K ~ responses x counts, likelihood for fixed noiseparams\n",
    "        \n",
    "        ##windows x 1         x counts  |\n",
    "        ##          responses x counts    tensordot\n",
    "        ##windows x responses\n",
    "        _oc_probs = T.matrix('oc_probs') ##N x K ~ this is a place holder for the \"object count prob\" matrix\n",
    "        _pred_dist = T.tensordot(_oc_probs, _lkhdTable, axes = [[1],[1]])\n",
    "        self.predictive_distribution_update_func = function(inputs=[_oc_probs,_lkhdTable], outputs=_pred_dist)\n",
    "       \n",
    "        \n",
    "    ##====initialization methods\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    ##select initial values of noiseparams, discretize, access lkhdCube\n",
    "    def init_noiseParams(self, pOnInit, pOffInit,noiseParamNumber):\n",
    "        '''\n",
    "        init_noiseParams(pOnInit, pOffInit,noiseParamNumber)\n",
    "        \n",
    "        create a grid of candidate noise params according noiseParamNumber (i.e., something like square root of number\n",
    "        of candidates we'll consider)\n",
    "        \n",
    "        take initial guess at noise param and return index of nearest candidate in the grid\n",
    "        \n",
    "        '''\n",
    "\n",
    "        ##will need the likelihood cube at whatever resolution we're using to infer noise params, so make here\n",
    "        self.noiseParamGrid = np.array(self.responses.noise.enumerate_param_grid(noiseParamNumber),dtype=floatX).T\n",
    "        \n",
    "        ##for starters, find index of closest noise param values to pOnInit, pOffInit, and then slice the lkhdCube\n",
    "        noiseParamIdx = np.argmin(map(np.linalg.norm, self.noiseParamGrid-np.array([pOnInit,pOffInit]).T))\n",
    "        return noiseParamIdx                                      \n",
    "       \n",
    "    \n",
    "    def init_qA(self):\n",
    "        '''\n",
    "        returns qA ~ 2 x numActivationVars array of activation probabilities\n",
    "        sampled from the prior.\n",
    "        for each var, qA[0] = ON probability, qA[1] = 1-qA[0] = OFF probability\n",
    "        '''\n",
    "        activationPriorProb = self.responses.Z.activations.activationPrior.activationProb\n",
    "        nAvars = self.responses.Z.activations.numActivationVars\n",
    "        nAstates = self.responses.Z.activations.numActivationStates ##this is always 2\n",
    "        \n",
    "        qA = np.empty((nAstates,nAvars),dtype=floatX)\n",
    "        qA[0,:] = activationPriorProb\n",
    "        qA[1,:] = 1-activationPriorProb\n",
    "        return qA\n",
    "    \n",
    "    def init_qt(self):\n",
    "        '''\n",
    "        returns qt ~ numIterationStates x numIterationVars array of iteration probabilities\n",
    "\n",
    "        sets values for numIterationStates. this truncates the support of the variational posterior\n",
    "        over iterations. bigger numbers = more time, so this will be treated as a hyperparameter.\n",
    "\n",
    "        to initialize, we evaluate probabilities under Poission prior for numbers of iterations up to\n",
    "        numIterationStates. then we normalize. so this is like a \"truncated possion\" starting qt.\n",
    "        \n",
    "        sets values for numIterationStates. this truncates the support of the variational posterior\n",
    "        over iterations. bigger numbers = more time, so this will be treated as a parameter for learning.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        nTvars = self.responses.Z.iterations.numIterationVars\n",
    "        nTstates = self.responses.Z.iterations.numIterationStates \n",
    "        qt = np.empty((nTstates,nTvars), dtype=floatX)\n",
    "        for v in range(nTvars):\n",
    "            qt[:,v] = self.responses.Z.iterations.iterationPrior.pmf(range(nTstates))\n",
    "            qt[:,v] /= np.sum(qt[:,v])\n",
    "        return qt\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    ##==========update and optimization methods=============\n",
    "    \n",
    "    def optimize_hyper_parameters(self):\n",
    "        '''\n",
    "        loop over all stored models and return the one with the best performance\n",
    "        (currently this is best percent correct)\n",
    "        '''\n",
    "        bestPercentCorrect = 0\n",
    "        bestK = np.inf\n",
    "        bestD = np.inf\n",
    "        for model in self.storedModels.values():\n",
    "            ##if this model is better than the best, promote it to new best\n",
    "            if model.bestPercentCorrect > bestPercentCorrect:\n",
    "                bestPercentCorrect = model.bestPercentCorrect\n",
    "                bestModel = model\n",
    "                bestK = model.responses.Z.categoryPrior.numObjects.K\n",
    "                bestD = model.responses.Z.numPixels.D\n",
    "            ##if this model is tied for best has fewer objects or pixels, promote it to new best    \n",
    "            elif model.bestPercentCorrect == bestPercentCorrect:\n",
    "                if (model.responses.Z.categoryPrior.numObjects.K < bestK) or (model.responses.Z.numPixels.D < bestD):\n",
    "                    bestPercentCorrect = model.bestPercentCorrect\n",
    "                    bestModel = model\n",
    "                    bestK = model.responses.Z.categoryPrior.numObjects.K\n",
    "                    bestD = model.responses.Z.numPixels.D\n",
    "        return bestModel\n",
    "            \n",
    "    \n",
    "    def update_qA(self, qA, qt, noiseParamStarIdx):\n",
    "        '''\n",
    "        coordinate ascent on variational posteriors of distortion activations\n",
    "        \n",
    "        update_qA(qA, qt, noiseParamStarIdx)\n",
    "        \n",
    "        returns numActivationStates x numActivationVars matrix of posterior probabilities\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        ##PStar is our term for the empirical lkhd cube sliced at noiseParamStarIdx. it is N x K\n",
    "        logPStar = np.log(self.curEmpiricalLkhdCube[noiseParamStarIdx])\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        nAvars = self.responses.Z.activations.numActivationVars\n",
    "        nAstates = self.responses.Z.activations.numActivationStates ##this is always 2\n",
    "        \n",
    "        #2 x N x K\n",
    "        oc_probs = np.zeros((nAstates, self.curN, K),dtype=floatX)\n",
    "        \n",
    "        #2 x 1 -- activation state * log[prior probability of activation]\n",
    "        v = np.zeros((nAstates,1), dtype=floatX)\n",
    "        #case where activation state is on we have 1*log[activationProb]\n",
    "        v[0] = np.log(self.responses.Z.activations.activationPrior.activationProb)\n",
    "        #case where it's off we have 0*log[1-activationProb]\n",
    "        v[1] = 0\n",
    "        \n",
    "        for n in range(nAvars):\n",
    "            \n",
    "            ##to sample, we only need the first column of qA, because that is the ON state\n",
    "            qA_copy = np.copy(qA)\n",
    "            \n",
    "            ##==activation = ON\n",
    "            qA_copy[0,n] = 1. ##set prob of ON to 1\n",
    "            qA_copy[1,n] = 0  ##set prob of OFF to 0\n",
    "            #M x K x D\n",
    "            sampledZ,_,_ = self.responses.Z.sample(M=self.numSamples, activationProbs=qA_copy, iterationProbs=qt)\n",
    "            #M x N\n",
    "            oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "            oc_probs[0,:,:] = self.object_count_prob_func(oc_counts,K)\n",
    "\n",
    "            ##==activation = OFF\n",
    "            qA_copy[0,n] = 0. ##set prob of ON to 0\n",
    "            qA_copy[1,n] = 1.  ##set prob of OFF to 1\n",
    "\n",
    "            #M x K x D\n",
    "            sampledZ,_,_ = self.responses.Z.sample(M=self.numSamples, activationProbs=qA_copy, iterationProbs=qt)\n",
    "            #M x N\n",
    "            oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "            #nAstates x N x K\n",
    "            oc_probs[1,:,:] = self.object_count_prob_func(oc_counts,K)\n",
    "            \n",
    "            ##===update qA for the n_th distortion\n",
    "            #nAstates x 1\n",
    "            qA[:, n] = np.clip(self.iQA._update_func(oc_probs, logPStar, v).squeeze(), self._a_min, self._a_max)\n",
    "        return qA\n",
    "\n",
    "    def update_qt(self, qA, qt, noiseParamStarIdx):\n",
    "        '''\n",
    "        coordinate ascent on variational posteriors of distortion iteration\n",
    "        \n",
    "        update_qt(qA, qt, noiseParamStarIdx)\n",
    "        \n",
    "        returns qt ~ numIterationVars x numIterationStates\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        ##PStar is our term for the empirical lkhd cube sliced at noiseParamStarIdx. it is N x K\n",
    "        logPStar = np.log(self.curEmpiricalLkhdCube[noiseParamStarIdx])\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        nTvars = self.responses.Z.iterations.numIterationVars\n",
    "        nTstates = self.responses.Z.iterations.numIterationStates ##a hyperparameter that user must determine\n",
    "        \n",
    "        #nTstates x N x K\n",
    "        oc_probs = np.zeros((nTstates, self.curN, K),dtype=floatX)\n",
    "\n",
    "        #nTstates x 1 log[prior probability of iteration value]\n",
    "        v = np.empty((nTstates,1), dtype=floatX)\n",
    "        v[:,0] = np.log(self.responses.Z.iterations.iterationPrior.pmf(range(nTstates)))\n",
    "        \n",
    "\n",
    "        for n in range(nTvars):\n",
    "            for t in range(nTstates):\n",
    "                ##number of iterations is t+1. \n",
    "                ##set probability of current iteration variable to 1\n",
    "                qt_copy = np.copy(qt)\n",
    "                qt_copy[:,n] = 0.\n",
    "                qt_copy[t,n] = 1.\n",
    "                #M x K x D\n",
    "                sampledZ,_,_ = self.responses.Z.sample(M=self.numSamples, activationProbs=qA, iterationProbs=qt_copy)\n",
    "                #M x N\n",
    "                oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "                \n",
    "                ##nTstates x N x K\n",
    "                oc_probs[t,:,:] = self.object_count_prob_func(oc_counts,K)\n",
    "                \n",
    "            #update qt for the n_th distortion\n",
    "            qt[:, n] = np.clip(self.iQt._update_func(oc_probs, logPStar, v).squeeze(), self._a_min, self._a_max)\n",
    "        return qt    \n",
    "    \n",
    "    \n",
    "\n",
    "    def update_goodness_of_fit(self, qA,qt):\n",
    "        '''\n",
    "        update_goodness_of_fit(qA,qt)\n",
    "        \n",
    "        return G x 1 array of measures of how well the variational posterior q(A,t) matches the data\n",
    "        for each of the G possible values of the noise params.\n",
    "        \n",
    "        '''\n",
    "        empiricalLkhdCube = self.curEmpiricalLkhdCube\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K\n",
    "        \n",
    "        sampledZ,_,_ = self.responses.Z.sample(M=self.numSamples, activationProbs=qA,iterationProbs=qt)\n",
    "        oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "        oc_probs = self.object_count_prob_func(oc_counts, K)\n",
    "        \n",
    "        goodnessOfFit = self.goodness_of_fit_func(empiricalLkhdCube, oc_probs)\n",
    "        \n",
    "        return goodnessOfFit\n",
    "            \n",
    "                                    \n",
    "    def optimize_PStar(self,qA,qt):\n",
    "        '''\n",
    "        optimize_PStar(qA,qt)\n",
    "        \n",
    "        PStar is what we call the empirical lkhd cube sliced at the current best noise params.\n",
    "        So, it is our current guess at the empricial lkhd table.\n",
    "        \n",
    "        We find the best noise param (by measuring goodness of fit)\n",
    "        '''\n",
    "        goodnessOfFit = self.update_goodness_of_fit(qA,qt)\n",
    "        goodnessOfFitStar, noiseParamStarIdx = self.oNP.update_noiseParams(goodnessOfFit)\n",
    "\n",
    "        ##N x K \n",
    "        PStar = self.curEmpiricalLkhdCube[noiseParamStarIdx]\n",
    "        \n",
    "        #1 x 2 best noiseparams\n",
    "        noiseParamStar = self.noiseParamGrid[noiseParamStarIdx,:]\n",
    "        return noiseParamStar, noiseParamStarIdx, PStar, goodnessOfFitStar\n",
    "    \n",
    "  \n",
    "\n",
    "        \n",
    "    ##===criticism===        \n",
    "    def update_ELBO(self, qA, qt, goodnessOfFitStar):\n",
    "        posterior_entropy = self.entropy_update_func(qA,qt)\n",
    "        _, _, ELBO = self.ELBO_update_func(np.asscalar(posterior_entropy),np.asscalar(goodnessOfFitStar))\n",
    "        return goodnessOfFitStar, posterior_entropy, ELBO\n",
    "        \n",
    "    def update_log_predictive_distribution(self, qA, qt, noiseParamStarIdx, numSamples=None):\n",
    "        '''\n",
    "        update_log_predictive_distribution(qA, qt, noiseParamStarIdx)\n",
    "        inputs:\n",
    "            qA ~ numActivationStates x numActivationVars\n",
    "            qt ~ numIterationsStates x numIterationVars\n",
    "            noiseParamStarIDx ~ int\n",
    "        \n",
    "        outputs:\n",
    "            predictiveDistribution ~ N x K+1 distribution over the K+1 possible response to each of the N windows\n",
    "            empiricalLogPredictiveDistribution ~ N x 1, this is log of predictiveDistribution sliced at empirical responses \n",
    "        '''\n",
    "        ##handle kwargs\n",
    "        if numSamples is None:\n",
    "            numSamples = self.numSamples\n",
    "        \n",
    "        ##create object count probabilities\n",
    "        K = self.responses.Z.categoryPrior.numObjects.K       \n",
    "        sampledZ,_,_ = self.responses.Z.sample(M=self.numSamples, activationProbs=qA, iterationProbs=qt)\n",
    "        oc_counts = self.responses.compute_feature(sampledZ,winIdx=self.curIdx)\n",
    "        oc_probs = self.object_count_prob_func(oc_counts, K)\n",
    "        \n",
    "        ##N x K+1, a distribution over K+1 responses to each window\n",
    "        predictiveDistribution = self.predictive_distribution_update_func(oc_probs,self.lkhdCube[noiseParamStarIdx])\n",
    "        \n",
    "        ##slice the predictive  distribution at the actual responses. if response exceeds capacity of model, prob. is 0\n",
    "        ##note that responses can be used as indices without change (unlike counts)\n",
    "        responsesAsIndices = self.curResponses.astype(intX)\n",
    "        smallEnough = map(lambda x: x <= K, responsesAsIndices)\n",
    "        empiricalPredictiveDistribution = np.where(smallEnough, predictiveDistribution[range(self.curN), responsesAsIndices.clip(0,K)],0)\n",
    "        \n",
    "#         empiricalPredictiveDistribution[smallEnough,:] = predictiveDistribution[smallEnough, responsesAsIndices[smallEnough]] ##(N x 1)\n",
    "        \n",
    "        ##N x 1\n",
    "        logEmpiricalPredictiveDistribution = np.log(empiricalPredictiveDistribution)\n",
    "        return predictiveDistribution, logEmpiricalPredictiveDistribution    \n",
    "    \n",
    "    \n",
    "    def update_percent_correct(self, predictiveDistribution):\n",
    "        \n",
    "        responses = self.curResponses.astype(intX)\n",
    "        predictions = np.argmax(predictiveDistribution, axis=1)\n",
    "        fraction_correct = np.sum(responses==predictions) / (self.curN*1.)\n",
    "        return fraction_correct*100, predictions\n",
    "    \n",
    "    def criticize(self, qA, qt, noiseParamStarIdx, goodnessOfFitStar, t):\n",
    "        '''\n",
    "        collects various performance metrics. saves them to training history arrays (which are attributes of the VI object)\n",
    "        notices when a training step produces a new, better solution, saves the model parameters when this happens\n",
    "        '''\n",
    "        self.goodnessOfFitStar_history[t] = goodnessOfFitStar\n",
    "        _,self.posteriorEntropy_history[t], self.ELBO_history[t] = self.update_ELBO(qA, qt, goodnessOfFitStar)\n",
    "        \n",
    "        ##get predictive distribution\n",
    "        predictiveDistribution,logEmpiricalPredictiveDistribution = self.update_log_predictive_distribution(qA, qt, noiseParamStarIdx)\n",
    "        \n",
    "        ##sum to get probability of observed data under predictive distribution\n",
    "        self.lnPredictiveDistribution_history[t] = logEmpiricalPredictiveDistribution.mean()\n",
    "        \n",
    "        ##use predicitive distribution to calculate percent correct\n",
    "        self.percentCorrect_history[t],_ = self.update_percent_correct(predictiveDistribution)\n",
    "        \n",
    "        if self.percentCorrect_history[t] > self.bestPercentCorrect:\n",
    "            print '!new best!'\n",
    "            self.bestQA = qA.copy()\n",
    "            self.bestQt = qt.copy()\n",
    "            self.bestNoiseParam = self.noiseParamGrid[noiseParamStarIdx,:].copy()\n",
    "            self.bestPercentCorrect = self.percentCorrect_history[t]\n",
    "            self.bestlnPredictiveDistribution = self.lnPredictiveDistribution_history[t]\n",
    "        \n",
    "\n",
    "        print 'ELBO: %f' %(self.ELBO_history[t])\n",
    "        print 'goodness of fit: %f' %(goodnessOfFitStar)\n",
    "        print 'posterior_entropy: %f' %(self.posteriorEntropy_history[t])\n",
    "        print 'mean log of predictive distribution over test samples: %f' %(self.lnPredictiveDistribution_history[t])\n",
    "        print 'percent correct over test samples: %f' %(self.percentCorrect_history[t])\n",
    "        print '\\n'\n",
    "        \n",
    "        return \n",
    "    \n",
    "        \n",
    "    ##======utilities and bookeeping\n",
    "    def rng_seed(self):\n",
    "        '''\n",
    "        sets (and resets) the seed for the random number generator\n",
    "        stores the seed as an attribute \"randNumberSeed\"\n",
    "        '''\n",
    "        ##if no seed yet, set one\n",
    "        if not hasattr(self, 'randNumberSeed'):\n",
    "            self.randNumberSeed = np.random.randint(0,high=10**4)\n",
    "        ##reset the seed so that subsequent calls to np.random.whatever replicate previous calls\n",
    "        np.random.seed(self.randNumberSeed)\n",
    "    \n",
    "    def train_test_regularize_splits(self, trainTestSplit, trainRegSplit):\n",
    "        N = self.responses.observations.shape[0]\n",
    "        shuffledIdx = np.random.permutation(N)\n",
    "        lastTestIdx = np.floor((1-trainTestSplit)*N).astype(intX)\n",
    "        lastRegIdx = lastTestIdx+np.floor((1-trainRegSplit)*(N-lastTestIdx)).astype(intX)\n",
    "    \n",
    "        testIdx = shuffledIdx[:lastTestIdx]\n",
    "        regIdx = shuffledIdx[lastTestIdx:lastRegIdx]\n",
    "        trainIdx = shuffledIdx[lastRegIdx:]\n",
    "        return trainIdx, testIdx, regIdx\n",
    "    \n",
    "    def empirical_lkhd_cube(self, idx=None):\n",
    "        if idx is None:\n",
    "            respAsIdx = self.responses.observations.astype(intX)\n",
    "        else:\n",
    "            respAsIdx = self.responses.observations[idx].astype(intX)    \n",
    "        return np.squeeze(self.lkhdCube[:,respAsIdx,:])\n",
    "        \n",
    "    def update_current(self, idx):\n",
    "        self.curN = len(idx)\n",
    "        self.curIdx = idx\n",
    "        self.curIdx = self.curIdx\n",
    "        self.curResponses = self.responses.observations[self.curIdx]\n",
    "        try: ##because the computation graph is inelegant\n",
    "            self.curEmpiricalLkhdCube = self.empirical_lkhd_cube(idx=self.curIdx)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    ##=============RUN the actual variational inference algorithm========\n",
    "    \n",
    "    ##Note: the only arguments passed to the various methods should be those that are updated every iteration.\n",
    "    def run_VI(self, pOnInit, pOffInit, noiseParamNumber, numSamples, maxEpochs, trainTestSplit, trainRegSplit):\n",
    "      \n",
    "        \n",
    "        ##divide the training, testing, regularization datasets\n",
    "        trainIdx, testIdx, regIdx = self.train_test_regularize_splits(trainTestSplit=trainTestSplit, trainRegSplit=trainRegSplit)\n",
    "\n",
    "        ##set empirical likelihoods, and responses to training set\n",
    "        self.update_current(trainIdx)\n",
    "        \n",
    "        \n",
    "        ##--construct the lkhdCube and slice it at the discrete value of the noise params closest to the supplied init values\n",
    "        noiseParamStarIdx = self.init_noiseParams(pOnInit, pOffInit, noiseParamNumber)\n",
    "\n",
    "        ##store numSamples for consultation in a few functions\n",
    "        self.numSamples = numSamples\n",
    "\n",
    "        ##--construct lkhd cube\n",
    "        self.lkhdCube = self.responses.make_lkhd_cube(self.noiseParamGrid[:,0],self.noiseParamGrid[:,1])  ##G x K x K\n",
    "\n",
    "        ##set current empirical likelihoods, and responses to training set (have to re-run because of lkhdCube)\n",
    "        self.update_current(trainIdx)\n",
    "\n",
    "        ##initialize variational posteriors\n",
    "        qA = self.init_qA()\n",
    "        qt = self.init_qt()\n",
    "\n",
    "        ##get initial goodnessOfFitStar\n",
    "        goodnessOfFitStar = self.update_goodness_of_fit(qA,qt)[noiseParamStarIdx]\n",
    "\n",
    "        ##set initial PStar, which is our term for an empirical lkhd table evaluated at the noiseParamStarIdx\n",
    "        ##PStar ~ N x K\n",
    "        PStar = self.curEmpiricalLkhdCube[noiseParamStarIdx]\n",
    "\n",
    "        ##--initialize arrays for learning histories and storing solutions\n",
    "        epoch = 0\n",
    "        self.ELBO_history = np.zeros((maxEpochs+1,1))\n",
    "        self.goodnessOfFitStar_history = np.zeros((maxEpochs+1,1))\n",
    "        self.posteriorEntropy_history = np.zeros((maxEpochs+1,1))\n",
    "        self.percentCorrect_history = np.zeros((maxEpochs+1,1))\n",
    "        self.lnPredictiveDistribution_history = np.zeros((maxEpochs+1,1))\n",
    "        self.bestPercentCorrect = 0\n",
    "\n",
    "        delta_ELBO = np.inf\n",
    "        min_delta_ELBO = 10e-15\n",
    "        ELBO_old = 0.\n",
    "\n",
    "        ##--publish initial criticism. use regularization data\n",
    "        self.update_current(regIdx)\n",
    "        self.criticize(qA,qt, noiseParamStarIdx, goodnessOfFitStar, epoch)\n",
    "\n",
    "        ##switch back to training data\n",
    "        self.update_current(trainIdx)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        ##this is the heart of the algorithm where the posteriors are updated\n",
    "        while (delta_ELBO > min_delta_ELBO) and (epoch <= maxEpochs):\n",
    "\n",
    "            ##put lkhd in log domain\n",
    "            lnPStar = np.log(PStar).astype(floatX)\n",
    "\n",
    "            ##coordinate ascent on variational posteriors of distortion activations/iterations\n",
    "            qA = self.update_qA(qA, qt, noiseParamStarIdx)\n",
    "            qt = self.update_qt(qA,qt, noiseParamStarIdx)\n",
    "\n",
    "            ##update noise params: calculates goodness of fit for all possible noise params, finds best,\n",
    "            ##returns it, returns best noiseParam, index of best noiseParam, and updates PStar\n",
    "            noiseParamStar, noiseParamStarIdx, PStar, goodnessOfFitStar = self.optimize_PStar(qA,qt)\n",
    "\n",
    "            ##criticize using regularization data\n",
    "            self.update_current(regIdx)\n",
    "            self.criticize(qA,qt,noiseParamStarIdx, goodnessOfFitStar, epoch)\n",
    "\n",
    "            ##switch back to training data\n",
    "            self.update_current(trainIdx)\n",
    "\n",
    "            ##update ELBO convergence criteria\n",
    "            delta_ELBO = np.abs(self.ELBO_history[epoch]-ELBO_old)\n",
    "            ELBO_old = self.ELBO_history[epoch]\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        self.trainIdx = trainIdx\n",
    "        self.regIdx = regIdx\n",
    "        self.testIdx = testIdx\n",
    "        return epoch\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========================SANDBOX==================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actP = activationPrior()\n",
    "actP.set_value(0.1)\n",
    "print actP.activationProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f643514fed0>]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64822b3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterP = iterationPrior()\n",
    "iterP.set_value(1.)\n",
    "plt.plot(range(10), iterP.pmf(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1,D2 = (64,64)\n",
    "smallMap = targetObjectMap.resize((D1,D2))\n",
    "bmo = baseObjectMap(smallMap)\n",
    "print bmo.objectCenters\n",
    "plt.imshow(bmo.objectMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpI,cpJ = np.meshgrid(np.linspace(12,D2-12,num=3), np.linspace(12,D1-12,num=3))\n",
    "controlPoints = np.array([cpI.flatten(),cpJ.flatten()],dtype=floatX).T\n",
    "globalAffineParamDictList = global_affine_distortion_set()\n",
    "gad = distortions(controlPoints,kind='global',distortionParamDictList=globalAffineParamDictList)\n",
    "print gad.numControlPoints\n",
    "print gad.numDistortions\n",
    "\n",
    "act = activations(actP,gad)\n",
    "actPval = np.empty((act.numActivationStates,act.numActivationVars))\n",
    "actPval[0,:] = .1  #ON state\n",
    "actPval[1,:] = .9  ##OFF state\n",
    "M = 10000\n",
    "samp1hot = act.sample(M=M, pval=actPval)\n",
    "print 'ON prob is: %f, should be: %f' %(samp1hot[:,0,:].astype(floatX).sum()/(M*act.numActivationVars),actPval[0,0])\n",
    "print 'OFF prob is: %f, should be: %f' %(samp1hot[:,1,:].astype(floatX).sum()/(M*act.numActivationVars),actPval[1,0])\n",
    "sampCanonical = act.sample(M=M, pval=actPval, kind='canonical')\n",
    "print 'ON prob is: %f, should be: %f' %(sampCanonical.astype(floatX).mean(), actPval[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iterations(iterP,numIterationVars=1,numIterationStates=4)\n",
    "itPval = np.empty((it.numIterationStates,it.numIterationVars))\n",
    "\n",
    "itPval[0,:] = .1  #no iterations\n",
    "itPval[1,:] = .7  ##1 iterations\n",
    "itPval[2,:] = .1  ##2 iterations\n",
    "itPval[3,:] = .1  ##3 iterations\n",
    "\n",
    "M = 10000\n",
    "samp1hot = it.sample(M=M, pval=itPval)\n",
    "print samp1hot.shape\n",
    "\n",
    "for i,p in enumerate(itPval[:,0]):\n",
    "    print 'prob is: %f, should be: %f' %(samp1hot[:,i,:].astype(floatX).sum()/(M*it.numIterationVars),p)\n",
    "\n",
    "sampCanonical = it.sample(M=M, pval=itPval, kind='canonical')\n",
    "sampCanonical.shape\n",
    "_=plt.hist(sampCanonical,bins=[0,1,2,3,4],density=True, align='left', rwidth=.3)\n",
    "# for i,p in enumerate(pval[:,0]):\n",
    "#     print 'prob is: %f, should be: %f' %(samp1hot[:,i,:].astype(floatX).sum()/(M*it.numIterationVars),p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lom = latentObjectMap(act,it,bmo)\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lom._get_base_map())\n",
    "plt.subplot(1,3,2)\n",
    "warpedCanonical,actSample,itSample = lom.sample(M=100,activationProbs=actPval,iterationProbs=itPval, kind='canonical')\n",
    "plt.imshow(warpedCanonical[0])\n",
    "print actSample.shape\n",
    "print itSample.shape\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "warped1hot,actSample,itSample = lom.sample(M=100,activationProbs=actPval,iterationProbs=itPval, kind='1hot')\n",
    "print warped1hot.shape\n",
    "targetZ1hot = warped1hot[:1]\n",
    "_=lom.convert1hot_to_image(targetZ1hot,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##windows\n",
    "DPrime1,DPrime2 = 2*D1, 2*D2 ##this gives the \"native resolution\" of the windows\n",
    "shape = (DPrime1,DPrime2)\n",
    "baseShape = (8, 8) ##size in pixels of the smallest probes\n",
    "numScales = 4 ##number of probe sizes between smallest and native resolution\n",
    "stride = 1 ##how far each probe travels when constructing probes, as a fraction of probe size\n",
    "numRandProbes = 420 ##number of non-contiguous probes\n",
    "randProbeOrder = (3, 7) ##non-contig probes will contain this many patches (range of)\n",
    "windows = probes() ##instantiate a windows object\n",
    "W = windows.make_windows(shape, baseShape, numScales, stride, numRandProbes, randProbeOrder) ##create the windows\n",
    "\n",
    "##now, we want to downsample the windows to a more manageable \"working\" resolution.\n",
    "##to do this, we first calculate all of the downsamples that have integer dimensions and preserve the aspect ratio\n",
    "##we set workingScale=n to choose the nth smallest resolution as our working resolution. \n",
    "##NOTE: THIS DOESN'T REALLY WORK BECAUSE EACH NATIVE RESOLUTIONI NEEDS TO BE CLEANLY DIVISIBLE BY WORKING RESOLUTION.\n",
    "##FOR THE \"UPSAMPLING\" OF Z TO WORK. SO, BEST JUST TO WORK WITH NATIVE RESOLUTIONS THAT ARE POWERS OF SOME NUMBER.\n",
    "##LIKE, SAY, 2.\n",
    "#resolutions, workingResolution = windows.resolve(shape, workingScale=-1) \n",
    "\n",
    "##Given the above, we'll just set the working resolution by hand\n",
    "workingResolution = (64,64)\n",
    "\n",
    "##next, we downsample the windows, and set_value\n",
    "windows.set_value(windows.reshape(W, workingResolution),flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noize = noiseParams()\n",
    "\n",
    "##response object\n",
    "r = behavior(lom,noize)\n",
    "pon = 0.99\n",
    "poff = 0.001\n",
    "r.set_values(windows=windows)\n",
    "data = r.sample(targetZ1hot,pon,poff)\n",
    "r.set_values(data=data)\n",
    "K = r.Z._get_num_objects()\n",
    "\n",
    "print 'total observations: %d' %(r.N)\n",
    "_=plt.hist(r.observations,bins=range(0,K+2),rwidth = .5, align='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'variationalImageRegistration' object has no attribute '_a_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-458-3c19bc128205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miqt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferQZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moNP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizeNoiseParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariationalImageRegistration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miqA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miqt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moNP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m##inference algorithm parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-457-4d0f4c993481>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, behavior_inst, activationInferQZ_inst, iterationInferQZ_inst, optimizeNoiseParams_inst)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m##compile theano expressions, functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_object_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_predictive_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_goodness_of_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-457-4d0f4c993481>\u001b[0m in \u001b[0;36mcompile_entropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0m_qA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qA_holder'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##numActivationStates x numActivationVars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0m_qt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qt_holder'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#numIterationStates x numIterationVars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0ma_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0ma_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m##scalar: entropy of joint variational posterior over A, t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'variationalImageRegistration' object has no attribute '_a_min'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-457-4d0f4c993481>\u001b[0m(63)\u001b[0;36mcompile_entropy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     61 \u001b[0;31m        \u001b[0m_qA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qA_holder'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##numActivationStates x numActivationVars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m        \u001b[0m_qt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qt_holder'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#numIterationStates x numIterationVars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 63 \u001b[0;31m        \u001b[0ma_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m        \u001b[0ma_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     65 \u001b[0;31m        \u001b[0;31m##scalar: entropy of joint variational posterior over A, t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "iqA = inferQZ()\n",
    "iqt = inferQZ()\n",
    "oNP = optimizeNoiseParams()\n",
    "vi = variationalImageRegistration(r,iqA,iqt,oNP)\n",
    "\n",
    "##inference algorithm parameters\n",
    "pOn_init, pOff_init = .99, 0.001\n",
    "densityOfNoiseParamGrid = 50\n",
    "numSamplesForComputingObjectCountProbs = 4\n",
    "maxNumIterations = 30\n",
    "trainTestSplit = .9\n",
    "trainRegSplit = .8\n",
    "\n",
    "bestModel =vi.run_VI(\n",
    "                     pOn_init, pOff_init, \\\n",
    "                     densityOfNoiseParamGrid, \\\n",
    "                     numSamplesForComputingObjectCountProbs, \\\n",
    "                     maxNumIterations, \\\n",
    "                     trainTestSplit, trainRegSplit, \\\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==============================TOSSED============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
